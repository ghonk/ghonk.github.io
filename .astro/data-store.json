[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.15.9","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://ghonk.github.io\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false},\"legacy\":{\"collections\":false}}","content-config-digest","654dd276434d3e65","blog",["Map",11,12,32,33,50,51,68,69,147,148,165,166,184,185,200,201,346,347,455,456,474,475,493,494,511,512,528,529,545,546,563,564,580,581,597,598,614,615,632,633,650,651,668,669,686,687,704,705,721,722,738,739,755,756,772,773,789,790,806,807,825,826,843,844,860,861,878,879,896,897,914,915,931,932,949,950,966,967,983,984,1000,1001,1019,1020,1037,1038],"2014-9-1-analogical-learning-in-museum",{"id":11,"data":13,"filePath":21,"digest":22,"rendered":23,"legacyId":31},{"title":14,"excerpt":15,"tags":16,"season":17,"assets":18,"type":19,"external_url":20},"Rapid Learning in a Children’s Museum via Analogical Comparison","Paper on analogical learning at the Chicago Children's Museum accepted for publication at Cognitive Science","analogy, cognition, paper","fall 2014","none","pdf","/pdfs/manuscripts/Gentner-et-al-2016-Museum.pdf","src/content/blog/2014-9-1-analogical-learning-in-museum.md","644779352c052263",{"html":24,"metadata":25},"",{"headings":26,"localImagePaths":27,"remoteImagePaths":28,"frontmatter":29,"imagePaths":30},[],[],[],{"title":14,"excerpt":15,"tags":16,"season":17,"assets":18,"type":19,"external_url":20},[],"2014-9-1-analogical-learning-in-museum.md","2015-7-1-aera-catcon",{"id":32,"data":34,"filePath":40,"digest":41,"rendered":42,"legacyId":49},{"title":35,"excerpt":36,"tags":37,"season":38,"assets":18,"type":19,"external_url":39},"Promoting transfer and mastery of evolution concepts with category construction","Presented a paper on teaching for transfer in middle school science at the American Educational Research Association annual meeting","transfer, cognition, paper, category, construction","summer 2015","/pdfs/manuscripts/honke-et-al-2015.pdf","src/content/blog/2015-7-1-aera-catcon.md","6355749a2d975711",{"html":24,"metadata":43},{"headings":44,"localImagePaths":45,"remoteImagePaths":46,"frontmatter":47,"imagePaths":48},[],[],[],{"title":35,"excerpt":36,"tags":37,"season":38,"assets":18,"type":19,"external_url":39},[],"2015-7-1-aera-catcon.md","2016-7-1-cogsci2016",{"id":50,"data":52,"filePath":58,"digest":59,"rendered":60,"legacyId":67},{"title":53,"excerpt":54,"tags":55,"season":56,"assets":18,"type":19,"external_url":57},"Category Learning via Feature Switching","I gave a talk at CogSci 2016 on learning categories via feature switching. Here's the paper.","CogSci, category learning","summer 2016","/pdfs/manuscripts/honke-conaway-kurtz2016.pdf","src/content/blog/2016-7-1-cogsci2016.md","f56279ba2a89c537",{"html":24,"metadata":61},{"headings":62,"localImagePaths":63,"remoteImagePaths":64,"frontmatter":65,"imagePaths":66},[],[],[],{"title":53,"excerpt":54,"tags":55,"season":56,"assets":18,"type":19,"external_url":57},[],"2016-7-1-cogsci2016.md","2017-12-20-roa-lstm",{"id":68,"data":70,"body":74,"filePath":75,"digest":76,"rendered":77,"legacyId":146},{"title":71,"excerpt":71,"tags":72,"season":73,"type":9},"Experimenting with text generation to create novel Ferengi Rules of Acquisition","ml, datascience, startrek, LSTM","winter 2017","### This post presents an *evolving* attempt to generate novel Ferengi Rules of Acquisition with classic and contemporary approaches to natural language processing. \n\n![timely](/assets/imgs/roa.jpg)\n\n### The Ferengi are a species of uber-capitalists with a 1950's mentality re: social issues that exist in the Star Trek Universe. The Rules of Acquisition are one part governmental charter, one part values system. They're written as proverbs, so they're naturally short and there aren't a ton of them (~138 in the comprehensive Star Trek universe). In their [own words](https://en.wikipedia.org/wiki/Rules_of_Acquisition):\n\n> \"Every Ferengi business transaction is governed by 285 Rules of Acquisition to ensure a fair and honest deal for all parties concerned... well most of them anyway.\" - Quark, \"The Maquis: Part 1\". Star Trek: Deep Space Nine\n\n### This means it should be a bit of a challenge to train up an LSTM to generate novel and sensible additions. On the other hand---as you might expect from the socio-cultrual guidelines for a socially-backwards, profit-at-all-costs society---the themes of the Rules are limited, suggesting that a fairly vanilla LSTM might be able to a decent job.   \n\n_________\n\nFirst Attempt: This approach uses an LSTM to generate character-by-character predictions after training on the Rules as a solid block of text. This approach was *heavily* inspired by Jason Brownlee's blog [post](https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/). \n\n\n# LSTM on the Ferengi Rules of Acquisition\n\n## getting the rules\n\n#### load some packages\n\n\n```python\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef getMiddleColumnFromRow(row):\n    return row.findAll('td')[1].text.strip()\n```\n\n#### grab the text\n\n\n```python\nresponse = requests.get(\n     'http://memory-beta.wikia.com/wiki/Ferengi_Rules_of_Acquisition'\n)\n```\n\n\n```python\nsoup = BeautifulSoup(response.content,'lxml')\nrules = list(map(getMiddleColumnFromRow, soup.find('table').findAll('tr')))\nrules = rules[1:len(rules)]\n```\n\n#### clean up the text\n\n\n```python\nrules = [s.lower() for s in rules]\nrules_block = \" \".join(rules)\nrules_block = rules_block.replace('\"', '').replace('*', '').replace('[', '').replace(']', '')\n```\n\n#### build a dict with all the characters\n\n\n```python\nchars = sorted(list(set(rules_block)))\nchars_to_int = dict((c, i) for i, c in enumerate(chars))\n```\n\n#### basic text properties\n\n\n```python\nn_chars = len(rules_block)\nn_vocab = len(chars)\nprint(\"Total Characters: \", n_chars)\nprint(\"Total Vocab: \", n_vocab)\n```\n\n    Total Characters:  6155\n    Total Vocab:  36\n\n\n#### advanced properties / ins and outs\n\n\n```python\nseq_length = 100\ndataX = []\ndataY = []\nfor i in range(0, n_chars - seq_length, 1):\n\tseq_in = rules_block[i:i + seq_length]\n\tseq_out = rules_block[i + seq_length]\n\tdataX.append([chars_to_int[char] for char in seq_in])\n\tdataY.append(chars_to_int[seq_out])\nn_patterns = len(dataX)\nprint(\"Total Patterns: \", n_patterns)\n```\n\n    Total Patterns:  6055\n\n\n## set up the model\n\n\n```python\nimport numpy as np\nimport sys\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import LSTM\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.utils import np_utils\n```\n\n#### create input/output encodings\n\n\n```python\n# reshape X to be [samples, time steps, features]\nX = np.reshape(dataX, (n_patterns, seq_length, 1))\n# normalize\nX = X / float(n_vocab)\n# one hot encode the output variable\ny = np_utils.to_categorical(dataY)\n```\n\n#### set model params\n\n\n```python\n# define the LSTM model\nmodel = Sequential()\nmodel.add(LSTM(256, input_shape = (X.shape[1], X.shape[2])))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(y.shape[1], activation = 'softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n```\n\n#### setup checkpointing regime\n\n\n```python\n# define the checkpoint\nfilepath = \"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\ncallbacks_list = [checkpoint]\n```\n\n#### run the model\n\n\n```python\nmodel.fit(X, y, epochs = 60, batch_size = 128, callbacks = callbacks_list)\n```\n\n    Epoch 1/60\n    6016/6055 [============================>.] - ETA: 0s - loss: 0.4661\n    Epoch 00001: loss did not improve\n    6055/6055 [==============================] - 45s 7ms/step - loss: 0.4655\n    Epoch 2/60\n    6016/6055 [============================>.] - ETA: 0s - loss: 0.3908\n    Epoch 00002: loss improved from 0.42985 to 0.39136, saving model to weights-improvement-02-0.3914.hdf5\n    6055/6055 [==============================] - 41s 7ms/step - loss: 0.3914\n    Epoch 3/60\n    6016/6055 [============================>.] - ETA: 0s - loss: 0.3774\n    Epoch 00003: loss improved from 0.39136 to 0.37890, saving model to weights-improvement-03-0.3789.hdf5\n    6055/6055 [==============================] - 57s 9ms/step - loss: 0.3789\n    Epoch 4/60\n    6016/6055 [============================>.] - ETA: 0s - loss: 0.5022\n    Epoch 00004: loss did not improve\n    6055/6055 [==============================] - 64s 11ms/step - loss: 0.5020\n    Epoch 5/60\n    6016/6055 [============================>.] - ETA: 0s - loss: 0.3542\n    Epoch 00005: loss improved from 0.37890 to 0.35414, saving model to weights-improvement-05-0.3541.hdf5\n    6055/6055 [==============================] - 53s 9ms/step - loss: 0.3541\n\n...\n\n    Epoch 60/60\n    6016/6055 [============================>.] - ETA: 0s - loss: 2.8709\n    Epoch 00060: loss did not improve\n    \n    \u003Ckeras.callbacks.History at 0x7fc8fbad9e10>\n\n\n\n## checking out the result\n\n#### after ~200 epochs we see some progress\n\n\n```python\n# load the network weights\nfilename = \"weights-improvement-19-0.2142.hdf5\"\nmodel.load_weights(filename)\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n```\n\n\n```python\nint_to_char = dict((i, c) for i, c in enumerate(chars))\n\n# pick a random seed\nstart = np.random.randint(0, len(dataX)-1)\npattern = dataX[start]\nprint(\"Seed: \")\nprint(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n# generate characters\nfor i in range(1000):\n\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n\tx = x / float(n_vocab)\n\tprediction = model.predict(x, verbose = 0)\n\tindex = numpy.argmax(prediction)\n\tresult = int_to_char[index]\n\tseq_in = [int_to_char[value] for value in pattern]\n\tsys.stdout.write(result)\n\tpattern.append(index)\n\tpattern = pattern[1:len(pattern)]\nprint(\"\\nDone.\")\n```\n\n    Seed: \n    \" t ain't broke, don't fix it. deep down, everyone's a ferengi. no good deed ever goes unpunished. alw \"\n    ays get somebody else to do the lifting. never get into anything that you can't get out of. \n    a man is only worth the sum of his possessions. an angry man is an enemy, and a satisfied man is an ally.. \n    the less employees know about the cash flow, the smaller the share they can demand. \n    only a fool passes up a business opportunity. the more time they take deciding, the more money they will spend.\n    s tor tear your far dom ner rasi foomims ts neon aootn nn meeee. \n    teel if pollh n  eaat irdi yeu cen bbtat tat seth too canio met ss aoos rntr toer tsmk to necd anp llow waat it aoot fn aatinem. \n    m mere ssut  on betior lat s tele.t tooeins, wever teee  mrrorian tsene aacitsieni woane auuunens is erre \n    aesineess th neal you can beeter take a venetaoe tho merd nothods ianere thtnh torr aon bdsicesiit  lhver allon famile to \n    stald to shof t  is oo whe woust oo time tomk toonits in toe einhenier the merce thsesns in  lm yher ysur arrfdds bo  \n    lhver tekes. foead is the ienllfie iome tour presi seene's fothing\n    Done.\n\n\n#### at $loss \\leq .06$ we've overfit the data; closer inspection suggests that's *all* we're doing\n\n\n```python\n# load the network weights\nfilename = \"weights-improvement-54-0.0554.hdf5\"\nmodel.load_weights(filename)\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n```\n\n\n```python\nint_to_char = dict((i, c) for i, c in enumerate(chars))\n\n# pick a random seed\nstart = np.random.randint(0, len(dataX)-1)\npattern = dataX[start]\nprint(\"Seed: \")\nprint(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n# generate characters\nfor i in range(1000):\n\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n\tx = x / float(n_vocab)\n\tprediction = model.predict(x, verbose = 0)\n\tindex = numpy.argmax(prediction)\n\tresult = int_to_char[index]\n\tseq_in = [int_to_char[value] for value in pattern]\n\tsys.stdout.write(result)\n\tpattern.append(index)\n\tpattern = pattern[1:len(pattern)]\nprint(\"\\nDone.\")\n```\n\n    Seed: \n    \" etter suit than your own. the bigger the smile, the sharper the knife. never ask when you can take.  \"\n    never trust anybody taller than you. rate divided by time equals profit. (also known as the velocity of wealth.) \n    take joy from profit, and profit from joy. good customers are as rare as latinum. treasure them. there is no substitute for success.\n    free advice is seldom cheap. keep your lies consistent. the riskier the road, the greater the profit. \n    work is the best therapy-at least for your employees. win or lose, there's always hupyrian beetle snuff. \n    someone's always got bigger ears. ferengi are not responsible for the stupidity of other races. knowledge equals profit. \n    home is where the heart is, but the stars are made of latinum. every once in a while, declare peace. \n    it confuses the hell out of your enemies. if you break it, i'll charge you for it! beware of the vulcan greed for knowledge. \n    the flimsier the product, the higher the price. never let the competition know what you're thinking. \n    learn the customer's weaknesses, so that you can better take advantage of him. it ain't over 'til i\n    Done.\n\n\n\n```python\nint_to_char = dict((i, c) for i, c in enumerate(chars))\n\n# pick a random seed\nstart = np.random.randint(0, len(dataX)-1)\npattern = dataX[start]\nprint(\"Seed: \")\nprint(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n# generate characters\nfor i in range(1000):\n\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n\tx = x / float(n_vocab)\n\tprediction = model.predict(x, verbose = 0)\n\tindex = numpy.argmax(prediction)\n\tresult = int_to_char[index]\n\tseq_in = [int_to_char[value] for value in pattern]\n\tsys.stdout.write(result)\n\tpattern.append(index)\n\tpattern = pattern[1:len(pattern)]\nprint(\"\\nDone.\")\n```\n\n    Seed: \n    \"  a product. time, like latinum, is a highly limited commodity. more is good...all is better. always  \"\n    leave yourself an out. a wife is a luxury... a smart accountant a neccessity. \n    when the messenger comes to appropriate your profits, kill the messenger. \n    a wealthy man can afford everything except a conscience. never let doubt interfere with your lust for latinum. \n    when in doubt, lie. always inspect the merchandise before making a deal. if it ain't broke, don't fix it. \n    deep down, everyone's a ferengi. no good deed ever goes unpunished. always get somebody else to do the lifting. \n    never get into anything that you can't get out of. a man is only worth the sum of his possessions. \n    an angry man is an enemy, and a satisfied man is an ally.. \n    the less employees know about the cash flow, the smaller the share they can demand. only a fool passes up a business opportunity.\n    the more time they take deciding, the more money they will spend.s aon thee tour frodit. fewares anx your fond berine. \n    sometimes what you get yrer cace to kropifs ts meol tou  atpi der oe yorrhit  novn ts need t erll frrm das. door\n    Done.\n\n\n\n```python\nint_to_char = dict((i, c) for i, c in enumerate(chars))\n\n# pick a random seed\nstart = np.random.randint(0, len(dataX)-1)\npattern = dataX[start]\nprint(\"Seed: \")\nprint(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n# generate characters\nfor i in range(1000):\n\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n\tx = x / float(n_vocab)\n\tprediction = model.predict(x, verbose = 0)\n\tindex = numpy.argmax(prediction)\n\tresult = int_to_char[index]\n\tseq_in = [int_to_char[value] for value in pattern]\n\tsys.stdout.write(result)\n\tpattern.append(index)\n\tpattern = pattern[1:len(pattern)]\nprint(\"\\nDone.\")\n```\n\n    Seed: \n    \"  wealthy man can afford everything except a conscience. never let doubt interfere with your lust for \"\n     latinum. when in doubt, lie. always inspect the merchandise before making a deal. if it ain't broke, don't fix it. \n     deep down, everyone's a ferengi. no good deed ever goes unpunished. always get somebody else to do the lifting. \n     never get into anything that you can't get out of. a man is only worth the sum of his possessions. \n     an angry man is an enemy, and a satisfied man is an ally.. the less employees know about the cash flow, the smaller the share they can demand. \n     only a fool passes up a business opportunity. the more time they take deciding, the more money they will spend.s aon thee tour frodit. \n     fewares anx your fond berine. sometimes what you get yrer cace to kropifs ts meol tou  atpi der oe yorrhit  novn ts need t erll frrm das. \n     dooreit is arofee..o lore as iotioess taah a qeeetiin  s mere sew letgo  lety asmayse dut mv wiat yhu can to feo .\n     on ortr thomiod thee y aow you  mead  rriedts it ee wete ns an bufey. aot yhe siace nhere l cela t  is anoeddeeee tht olly.\n     on khew touk ansioee\n    Done.\n\nSo, the character-by-character approach clearly is not working for us---the model has only learned to produce a verbatim replication of the input and we don't get any of the neat novelty we were hoping for. Note: this is before any parameter tuning, choosing some sensible defaults (e.g., the `adam` optimizer, unit dropout) and setting the input to character-by-character. \n\nThe biggest concern with LSTM on this project was that the data were simply too small to build a generative representation. No intermediate step produced good rules---steps only differed in their quality of identical replication of the character sequence. The choice of input as 100 character blocks might be increasing this identical text replication pattern. On the other hand, word-by-word prediction will produce an even smaller set of patterns. Stay tuned for the next iteration where a word-by-word LSTM will be examined.","src/content/blog/2017-12-20-roa-lstm.md","54945249617ae68a",{"html":78,"metadata":79},"\u003Ch3 id=\"this-post-presents-an-evolving-attempt-to-generate-novel-ferengi-rules-of-acquisition-with-classic-and-contemporary-approaches-to-natural-language-processing\">This post presents an \u003Cem>evolving\u003C/em> attempt to generate novel Ferengi Rules of Acquisition with classic and contemporary approaches to natural language processing.\u003C/h3>\n\u003Cp>\u003Cimg src=\"/assets/imgs/roa.jpg\" alt=\"timely\">\u003C/p>\n\u003Ch3 id=\"the-ferengi-are-a-species-of-uber-capitalists-with-a-1950s-mentality-re-social-issues-that-exist-in-the-star-trek-universe-the-rules-of-acquisition-are-one-part-governmental-charter-one-part-values-system-theyre-written-as-proverbs-so-theyre-naturally-short-and-there-arent-a-ton-of-them-138-in-the-comprehensive-star-trek-universe-in-their-own-words\">The Ferengi are a species of uber-capitalists with a 1950’s mentality re: social issues that exist in the Star Trek Universe. The Rules of Acquisition are one part governmental charter, one part values system. They’re written as proverbs, so they’re naturally short and there aren’t a ton of them (~138 in the comprehensive Star Trek universe). In their \u003Ca href=\"https://en.wikipedia.org/wiki/Rules_of_Acquisition\">own words\u003C/a>:\u003C/h3>\n\u003Cblockquote>\n\u003Cp>“Every Ferengi business transaction is governed by 285 Rules of Acquisition to ensure a fair and honest deal for all parties concerned… well most of them anyway.” - Quark, “The Maquis: Part 1”. Star Trek: Deep Space Nine\u003C/p>\n\u003C/blockquote>\n\u003Ch3 id=\"this-means-it-should-be-a-bit-of-a-challenge-to-train-up-an-lstm-to-generate-novel-and-sensible-additions-on-the-other-hand---as-you-might-expect-from-the-socio-cultrual-guidelines-for-a-socially-backwards-profit-at-all-costs-society---the-themes-of-the-rules-are-limited-suggesting-that-a-fairly-vanilla-lstm-might-be-able-to-a-decent-job\">This means it should be a bit of a challenge to train up an LSTM to generate novel and sensible additions. On the other hand---as you might expect from the socio-cultrual guidelines for a socially-backwards, profit-at-all-costs society---the themes of the Rules are limited, suggesting that a fairly vanilla LSTM might be able to a decent job.\u003C/h3>\n\u003Chr>\n\u003Cp>First Attempt: This approach uses an LSTM to generate character-by-character predictions after training on the Rules as a solid block of text. This approach was \u003Cem>heavily\u003C/em> inspired by Jason Brownlee’s blog \u003Ca href=\"https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\">post\u003C/a>.\u003C/p>\n\u003Ch1 id=\"lstm-on-the-ferengi-rules-of-acquisition\">LSTM on the Ferengi Rules of Acquisition\u003C/h1>\n\u003Ch2 id=\"getting-the-rules\">getting the rules\u003C/h2>\n\u003Ch4 id=\"load-some-packages\">load some packages\u003C/h4>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">import\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> requests\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">from\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> bs4 \u003C/span>\u003Cspan style=\"color:#F97583\">import\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> BeautifulSoup\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">def\u003C/span>\u003Cspan style=\"color:#B392F0\"> getMiddleColumnFromRow\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(row):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    return\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> row.findAll(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">'td'\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)[\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">].text.strip()\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch4 id=\"grab-the-text\">grab the text\u003C/h4>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">response \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> requests.get(\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">     'http://memory-beta.wikia.com/wiki/Ferengi_Rules_of_Acquisition'\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">soup \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> BeautifulSoup(response.content,\u003C/span>\u003Cspan style=\"color:#9ECBFF\">'lxml'\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">rules \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> list\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#79B8FF\">map\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(getMiddleColumnFromRow, soup.find(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">'table'\u003C/span>\u003Cspan style=\"color:#E1E4E8\">).findAll(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">'tr'\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">rules \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> rules[\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003Cspan style=\"color:#79B8FF\">len\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(rules)]\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch4 id=\"clean-up-the-text\">clean up the text\u003C/h4>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">rules \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> [s.lower() \u003C/span>\u003Cspan style=\"color:#F97583\">for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> s \u003C/span>\u003Cspan style=\"color:#F97583\">in\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> rules]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">rules_block \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \" \"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.join(rules)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">rules_block \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> rules_block.replace(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">'\"'\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#9ECBFF\">''\u003C/span>\u003Cspan style=\"color:#E1E4E8\">).replace(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">'*'\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#9ECBFF\">''\u003C/span>\u003Cspan style=\"color:#E1E4E8\">).replace(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">'['\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#9ECBFF\">''\u003C/span>\u003Cspan style=\"color:#E1E4E8\">).replace(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">']'\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#9ECBFF\">''\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch4 id=\"build-a-dict-with-all-the-characters\">build a dict with all the characters\u003C/h4>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">chars \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> sorted\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#79B8FF\">list\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#79B8FF\">set\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(rules_block)))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">chars_to_int \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> dict\u003C/span>\u003Cspan style=\"color:#E1E4E8\">((c, i) \u003C/span>\u003Cspan style=\"color:#F97583\">for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> i, c \u003C/span>\u003Cspan style=\"color:#F97583\">in\u003C/span>\u003Cspan style=\"color:#79B8FF\"> enumerate\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(chars))\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch4 id=\"basic-text-properties\">basic text properties\u003C/h4>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">n_chars \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> len\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(rules_block)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">n_vocab \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> len\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(chars)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">print\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Total Characters: \"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, n_chars)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">print\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Total Vocab: \"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, n_vocab)\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>Total Characters:  6155\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>Total Vocab:  36\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch4 id=\"advanced-properties--ins-and-outs\">advanced properties / ins and outs\u003C/h4>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">seq_length \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 100\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">dataX \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> []\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">dataY \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> []\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> i \u003C/span>\u003Cspan style=\"color:#F97583\">in\u003C/span>\u003Cspan style=\"color:#79B8FF\"> range\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#79B8FF\">0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, n_chars \u003C/span>\u003Cspan style=\"color:#F97583\">-\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> seq_length, \u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tseq_in \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> rules_block[i:i \u003C/span>\u003Cspan style=\"color:#F97583\">+\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> seq_length]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tseq_out \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> rules_block[i \u003C/span>\u003Cspan style=\"color:#F97583\">+\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> seq_length]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tdataX.append([chars_to_int[char] \u003C/span>\u003Cspan style=\"color:#F97583\">for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> char \u003C/span>\u003Cspan style=\"color:#F97583\">in\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> seq_in])\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tdataY.append(chars_to_int[seq_out])\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">n_patterns \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> len\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(dataX)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">print\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Total Patterns: \"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, n_patterns)\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>Total Patterns:  6055\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch2 id=\"set-up-the-model\">set up the model\u003C/h2>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">import\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> numpy \u003C/span>\u003Cspan style=\"color:#F97583\">as\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> np\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">import\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> sys\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">from\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> keras.models \u003C/span>\u003Cspan style=\"color:#F97583\">import\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> Sequential\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">from\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> keras.layers \u003C/span>\u003Cspan style=\"color:#F97583\">import\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> Dense\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">from\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> keras.layers \u003C/span>\u003Cspan style=\"color:#F97583\">import\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> Dropout\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">from\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> keras.layers \u003C/span>\u003Cspan style=\"color:#F97583\">import\u003C/span>\u003Cspan style=\"color:#79B8FF\"> LSTM\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">from\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> keras.callbacks \u003C/span>\u003Cspan style=\"color:#F97583\">import\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> ModelCheckpoint\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">from\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> keras.utils \u003C/span>\u003Cspan style=\"color:#F97583\">import\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> np_utils\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch4 id=\"create-inputoutput-encodings\">create input/output encodings\u003C/h4>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># reshape X to be [samples, time steps, features]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">X \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> np.reshape(dataX, (n_patterns, seq_length, \u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># normalize\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">X \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> X \u003C/span>\u003Cspan style=\"color:#F97583\">/\u003C/span>\u003Cspan style=\"color:#79B8FF\"> float\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(n_vocab)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># one hot encode the output variable\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">y \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> np_utils.to_categorical(dataY)\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch4 id=\"set-model-params\">set model params\u003C/h4>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># define the LSTM model\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">model \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> Sequential()\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">model.add(LSTM(\u003C/span>\u003Cspan style=\"color:#79B8FF\">256\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">input_shape\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (X.shape[\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">], X.shape[\u003C/span>\u003Cspan style=\"color:#79B8FF\">2\u003C/span>\u003Cspan style=\"color:#E1E4E8\">])))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">model.add(Dropout(\u003C/span>\u003Cspan style=\"color:#79B8FF\">0.2\u003C/span>\u003Cspan style=\"color:#E1E4E8\">))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">model.add(Dense(y.shape[\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">], \u003C/span>\u003Cspan style=\"color:#FFAB70\">activation\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> 'softmax'\u003C/span>\u003Cspan style=\"color:#E1E4E8\">))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">model.compile(\u003C/span>\u003Cspan style=\"color:#FFAB70\">loss\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> 'categorical_crossentropy'\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">optimizer\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> 'adam'\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch4 id=\"setup-checkpointing-regime\">setup checkpointing regime\u003C/h4>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># define the checkpoint\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">filepath \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"weights-improvement-\u003C/span>\u003Cspan style=\"color:#79B8FF\">{epoch\u003C/span>\u003Cspan style=\"color:#F97583\">:02d\u003C/span>\u003Cspan style=\"color:#79B8FF\">}\u003C/span>\u003Cspan style=\"color:#9ECBFF\">-\u003C/span>\u003Cspan style=\"color:#79B8FF\">{loss\u003C/span>\u003Cspan style=\"color:#F97583\">:.4f\u003C/span>\u003Cspan style=\"color:#79B8FF\">}\u003C/span>\u003Cspan style=\"color:#9ECBFF\">.hdf5\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">checkpoint \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> ModelCheckpoint(filepath, \u003C/span>\u003Cspan style=\"color:#FFAB70\">monitor\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#9ECBFF\">'loss'\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">verbose\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">save_best_only\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">True\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">mode\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#9ECBFF\">'min'\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">callbacks_list \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> [checkpoint]\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch4 id=\"run-the-model\">run the model\u003C/h4>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">model.fit(X, y, \u003C/span>\u003Cspan style=\"color:#FFAB70\">epochs\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 60\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">batch_size\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 128\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">callbacks\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> callbacks_list)\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>Epoch 1/60\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>6016/6055 [============================>.] - ETA: 0s - loss: 0.4661\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>Epoch 00001: loss did not improve\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>6055/6055 [==============================] - 45s 7ms/step - loss: 0.4655\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>Epoch 2/60\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>6016/6055 [============================>.] - ETA: 0s - loss: 0.3908\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>Epoch 00002: loss improved from 0.42985 to 0.39136, saving model to weights-improvement-02-0.3914.hdf5\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>6055/6055 [==============================] - 41s 7ms/step - loss: 0.3914\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>Epoch 3/60\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>6016/6055 [============================>.] - ETA: 0s - loss: 0.3774\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>Epoch 00003: loss improved from 0.39136 to 0.37890, saving model to weights-improvement-03-0.3789.hdf5\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>6055/6055 [==============================] - 57s 9ms/step - loss: 0.3789\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>Epoch 4/60\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>6016/6055 [============================>.] - ETA: 0s - loss: 0.5022\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>Epoch 00004: loss did not improve\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>6055/6055 [==============================] - 64s 11ms/step - loss: 0.5020\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>Epoch 5/60\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>6016/6055 [============================>.] - ETA: 0s - loss: 0.3542\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>Epoch 00005: loss improved from 0.37890 to 0.35414, saving model to weights-improvement-05-0.3541.hdf5\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>6055/6055 [==============================] - 53s 9ms/step - loss: 0.3541\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>…\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>Epoch 60/60\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>6016/6055 [============================>.] - ETA: 0s - loss: 2.8709\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>Epoch 00060: loss did not improve\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>&#x3C;keras.callbacks.History at 0x7fc8fbad9e10>\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch2 id=\"checking-out-the-result\">checking out the result\u003C/h2>\n\u003Ch4 id=\"after-200-epochs-we-see-some-progress\">after ~200 epochs we see some progress\u003C/h4>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># load the network weights\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">filename \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"weights-improvement-19-0.2142.hdf5\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">model.load_weights(filename)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">model.compile(\u003C/span>\u003Cspan style=\"color:#FFAB70\">loss\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> 'categorical_crossentropy'\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">optimizer\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> 'adam'\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">int_to_char \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> dict\u003C/span>\u003Cspan style=\"color:#E1E4E8\">((i, c) \u003C/span>\u003Cspan style=\"color:#F97583\">for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> i, c \u003C/span>\u003Cspan style=\"color:#F97583\">in\u003C/span>\u003Cspan style=\"color:#79B8FF\"> enumerate\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(chars))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># pick a random seed\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">start \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> np.random.randint(\u003C/span>\u003Cspan style=\"color:#79B8FF\">0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">len\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(dataX)\u003C/span>\u003Cspan style=\"color:#F97583\">-\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">pattern \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> dataX[start]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">print\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Seed: \"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">print\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003Cspan style=\"color:#79B8FF\">\\\"\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#9ECBFF\">''\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.join([int_to_char[value] \u003C/span>\u003Cspan style=\"color:#F97583\">for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> value \u003C/span>\u003Cspan style=\"color:#F97583\">in\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> pattern]), \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003Cspan style=\"color:#79B8FF\">\\\"\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># generate characters\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> i \u003C/span>\u003Cspan style=\"color:#F97583\">in\u003C/span>\u003Cspan style=\"color:#79B8FF\"> range\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#79B8FF\">1000\u003C/span>\u003Cspan style=\"color:#E1E4E8\">):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tx \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> numpy.reshape(pattern, (\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">len\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(pattern), \u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tx \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> x \u003C/span>\u003Cspan style=\"color:#F97583\">/\u003C/span>\u003Cspan style=\"color:#79B8FF\"> float\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(n_vocab)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tprediction \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> model.predict(x, \u003C/span>\u003Cspan style=\"color:#FFAB70\">verbose\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tindex \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> numpy.argmax(prediction)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tresult \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> int_to_char[index]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tseq_in \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> [int_to_char[value] \u003C/span>\u003Cspan style=\"color:#F97583\">for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> value \u003C/span>\u003Cspan style=\"color:#F97583\">in\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> pattern]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tsys.stdout.write(result)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tpattern.append(index)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tpattern \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> pattern[\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003Cspan style=\"color:#79B8FF\">len\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(pattern)]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">print\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003Cspan style=\"color:#79B8FF\">\\n\u003C/span>\u003Cspan style=\"color:#9ECBFF\">Done.\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>Seed: \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>\" t ain't broke, don't fix it. deep down, everyone's a ferengi. no good deed ever goes unpunished. alw \"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>ays get somebody else to do the lifting. never get into anything that you can't get out of. \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>a man is only worth the sum of his possessions. an angry man is an enemy, and a satisfied man is an ally.. \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>the less employees know about the cash flow, the smaller the share they can demand. \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>only a fool passes up a business opportunity. the more time they take deciding, the more money they will spend.\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>s tor tear your far dom ner rasi foomims ts neon aootn nn meeee. \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>teel if pollh n  eaat irdi yeu cen bbtat tat seth too canio met ss aoos rntr toer tsmk to necd anp llow waat it aoot fn aatinem. \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>m mere ssut  on betior lat s tele.t tooeins, wever teee  mrrorian tsene aacitsieni woane auuunens is erre \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>aesineess th neal you can beeter take a venetaoe tho merd nothods ianere thtnh torr aon bdsicesiit  lhver allon famile to \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>stald to shof t  is oo whe woust oo time tomk toonits in toe einhenier the merce thsesns in  lm yher ysur arrfdds bo  \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>lhver tekes. foead is the ienllfie iome tour presi seene's fothing\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>Done.\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch4 id=\"at-loss-leq-06-weve-overfit-the-data-closer-inspection-suggests-thats-all-were-doing\">at $loss \\leq .06$ we’ve overfit the data; closer inspection suggests that’s \u003Cem>all\u003C/em> we’re doing\u003C/h4>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># load the network weights\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">filename \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"weights-improvement-54-0.0554.hdf5\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">model.load_weights(filename)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">model.compile(\u003C/span>\u003Cspan style=\"color:#FFAB70\">loss\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> 'categorical_crossentropy'\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">optimizer\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> 'adam'\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">int_to_char \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> dict\u003C/span>\u003Cspan style=\"color:#E1E4E8\">((i, c) \u003C/span>\u003Cspan style=\"color:#F97583\">for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> i, c \u003C/span>\u003Cspan style=\"color:#F97583\">in\u003C/span>\u003Cspan style=\"color:#79B8FF\"> enumerate\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(chars))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># pick a random seed\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">start \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> np.random.randint(\u003C/span>\u003Cspan style=\"color:#79B8FF\">0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">len\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(dataX)\u003C/span>\u003Cspan style=\"color:#F97583\">-\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">pattern \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> dataX[start]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">print\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Seed: \"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">print\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003Cspan style=\"color:#79B8FF\">\\\"\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#9ECBFF\">''\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.join([int_to_char[value] \u003C/span>\u003Cspan style=\"color:#F97583\">for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> value \u003C/span>\u003Cspan style=\"color:#F97583\">in\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> pattern]), \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003Cspan style=\"color:#79B8FF\">\\\"\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># generate characters\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> i \u003C/span>\u003Cspan style=\"color:#F97583\">in\u003C/span>\u003Cspan style=\"color:#79B8FF\"> range\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#79B8FF\">1000\u003C/span>\u003Cspan style=\"color:#E1E4E8\">):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tx \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> numpy.reshape(pattern, (\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">len\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(pattern), \u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tx \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> x \u003C/span>\u003Cspan style=\"color:#F97583\">/\u003C/span>\u003Cspan style=\"color:#79B8FF\"> float\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(n_vocab)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tprediction \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> model.predict(x, \u003C/span>\u003Cspan style=\"color:#FFAB70\">verbose\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tindex \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> numpy.argmax(prediction)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tresult \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> int_to_char[index]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tseq_in \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> [int_to_char[value] \u003C/span>\u003Cspan style=\"color:#F97583\">for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> value \u003C/span>\u003Cspan style=\"color:#F97583\">in\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> pattern]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tsys.stdout.write(result)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tpattern.append(index)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tpattern \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> pattern[\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003Cspan style=\"color:#79B8FF\">len\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(pattern)]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">print\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003Cspan style=\"color:#79B8FF\">\\n\u003C/span>\u003Cspan style=\"color:#9ECBFF\">Done.\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>Seed: \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>\" etter suit than your own. the bigger the smile, the sharper the knife. never ask when you can take.  \"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>never trust anybody taller than you. rate divided by time equals profit. (also known as the velocity of wealth.) \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>take joy from profit, and profit from joy. good customers are as rare as latinum. treasure them. there is no substitute for success.\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>free advice is seldom cheap. keep your lies consistent. the riskier the road, the greater the profit. \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>work is the best therapy-at least for your employees. win or lose, there's always hupyrian beetle snuff. \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>someone's always got bigger ears. ferengi are not responsible for the stupidity of other races. knowledge equals profit. \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>home is where the heart is, but the stars are made of latinum. every once in a while, declare peace. \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>it confuses the hell out of your enemies. if you break it, i'll charge you for it! beware of the vulcan greed for knowledge. \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>the flimsier the product, the higher the price. never let the competition know what you're thinking. \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>learn the customer's weaknesses, so that you can better take advantage of him. it ain't over 'til i\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>Done.\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">int_to_char \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> dict\u003C/span>\u003Cspan style=\"color:#E1E4E8\">((i, c) \u003C/span>\u003Cspan style=\"color:#F97583\">for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> i, c \u003C/span>\u003Cspan style=\"color:#F97583\">in\u003C/span>\u003Cspan style=\"color:#79B8FF\"> enumerate\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(chars))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># pick a random seed\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">start \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> np.random.randint(\u003C/span>\u003Cspan style=\"color:#79B8FF\">0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">len\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(dataX)\u003C/span>\u003Cspan style=\"color:#F97583\">-\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">pattern \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> dataX[start]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">print\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Seed: \"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">print\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003Cspan style=\"color:#79B8FF\">\\\"\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#9ECBFF\">''\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.join([int_to_char[value] \u003C/span>\u003Cspan style=\"color:#F97583\">for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> value \u003C/span>\u003Cspan style=\"color:#F97583\">in\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> pattern]), \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003Cspan style=\"color:#79B8FF\">\\\"\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># generate characters\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> i \u003C/span>\u003Cspan style=\"color:#F97583\">in\u003C/span>\u003Cspan style=\"color:#79B8FF\"> range\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#79B8FF\">1000\u003C/span>\u003Cspan style=\"color:#E1E4E8\">):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tx \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> numpy.reshape(pattern, (\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">len\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(pattern), \u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tx \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> x \u003C/span>\u003Cspan style=\"color:#F97583\">/\u003C/span>\u003Cspan style=\"color:#79B8FF\"> float\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(n_vocab)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tprediction \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> model.predict(x, \u003C/span>\u003Cspan style=\"color:#FFAB70\">verbose\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tindex \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> numpy.argmax(prediction)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tresult \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> int_to_char[index]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tseq_in \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> [int_to_char[value] \u003C/span>\u003Cspan style=\"color:#F97583\">for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> value \u003C/span>\u003Cspan style=\"color:#F97583\">in\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> pattern]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tsys.stdout.write(result)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tpattern.append(index)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tpattern \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> pattern[\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003Cspan style=\"color:#79B8FF\">len\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(pattern)]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">print\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003Cspan style=\"color:#79B8FF\">\\n\u003C/span>\u003Cspan style=\"color:#9ECBFF\">Done.\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>Seed: \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>\"  a product. time, like latinum, is a highly limited commodity. more is good...all is better. always  \"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>leave yourself an out. a wife is a luxury... a smart accountant a neccessity. \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>when the messenger comes to appropriate your profits, kill the messenger. \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>a wealthy man can afford everything except a conscience. never let doubt interfere with your lust for latinum. \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>when in doubt, lie. always inspect the merchandise before making a deal. if it ain't broke, don't fix it. \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>deep down, everyone's a ferengi. no good deed ever goes unpunished. always get somebody else to do the lifting. \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>never get into anything that you can't get out of. a man is only worth the sum of his possessions. \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>an angry man is an enemy, and a satisfied man is an ally.. \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>the less employees know about the cash flow, the smaller the share they can demand. only a fool passes up a business opportunity.\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>the more time they take deciding, the more money they will spend.s aon thee tour frodit. fewares anx your fond berine. \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>sometimes what you get yrer cace to kropifs ts meol tou  atpi der oe yorrhit  novn ts need t erll frrm das. door\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>Done.\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">int_to_char \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> dict\u003C/span>\u003Cspan style=\"color:#E1E4E8\">((i, c) \u003C/span>\u003Cspan style=\"color:#F97583\">for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> i, c \u003C/span>\u003Cspan style=\"color:#F97583\">in\u003C/span>\u003Cspan style=\"color:#79B8FF\"> enumerate\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(chars))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># pick a random seed\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">start \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> np.random.randint(\u003C/span>\u003Cspan style=\"color:#79B8FF\">0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">len\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(dataX)\u003C/span>\u003Cspan style=\"color:#F97583\">-\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">pattern \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> dataX[start]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">print\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Seed: \"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">print\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003Cspan style=\"color:#79B8FF\">\\\"\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#9ECBFF\">''\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.join([int_to_char[value] \u003C/span>\u003Cspan style=\"color:#F97583\">for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> value \u003C/span>\u003Cspan style=\"color:#F97583\">in\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> pattern]), \u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003Cspan style=\"color:#79B8FF\">\\\"\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># generate characters\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> i \u003C/span>\u003Cspan style=\"color:#F97583\">in\u003C/span>\u003Cspan style=\"color:#79B8FF\"> range\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#79B8FF\">1000\u003C/span>\u003Cspan style=\"color:#E1E4E8\">):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tx \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> numpy.reshape(pattern, (\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">len\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(pattern), \u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tx \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> x \u003C/span>\u003Cspan style=\"color:#F97583\">/\u003C/span>\u003Cspan style=\"color:#79B8FF\"> float\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(n_vocab)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tprediction \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> model.predict(x, \u003C/span>\u003Cspan style=\"color:#FFAB70\">verbose\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tindex \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> numpy.argmax(prediction)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tresult \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> int_to_char[index]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tseq_in \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> [int_to_char[value] \u003C/span>\u003Cspan style=\"color:#F97583\">for\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> value \u003C/span>\u003Cspan style=\"color:#F97583\">in\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> pattern]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tsys.stdout.write(result)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tpattern.append(index)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\tpattern \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> pattern[\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003Cspan style=\"color:#79B8FF\">len\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(pattern)]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">print\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003Cspan style=\"color:#79B8FF\">\\n\u003C/span>\u003Cspan style=\"color:#9ECBFF\">Done.\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>Seed: \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>\"  wealthy man can afford everything except a conscience. never let doubt interfere with your lust for \"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan> latinum. when in doubt, lie. always inspect the merchandise before making a deal. if it ain't broke, don't fix it. \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan> deep down, everyone's a ferengi. no good deed ever goes unpunished. always get somebody else to do the lifting. \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan> never get into anything that you can't get out of. a man is only worth the sum of his possessions. \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan> an angry man is an enemy, and a satisfied man is an ally.. the less employees know about the cash flow, the smaller the share they can demand. \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan> only a fool passes up a business opportunity. the more time they take deciding, the more money they will spend.s aon thee tour frodit. \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan> fewares anx your fond berine. sometimes what you get yrer cace to kropifs ts meol tou  atpi der oe yorrhit  novn ts need t erll frrm das. \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan> dooreit is arofee..o lore as iotioess taah a qeeetiin  s mere sew letgo  lety asmayse dut mv wiat yhu can to feo .\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan> on ortr thomiod thee y aow you  mead  rriedts it ee wete ns an bufey. aot yhe siace nhere l cela t  is anoeddeeee tht olly.\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan> on khew touk ansioee\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>Done.\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>So, the character-by-character approach clearly is not working for us---the model has only learned to produce a verbatim replication of the input and we don’t get any of the neat novelty we were hoping for. Note: this is before any parameter tuning, choosing some sensible defaults (e.g., the \u003Ccode>adam\u003C/code> optimizer, unit dropout) and setting the input to character-by-character.\u003C/p>\n\u003Cp>The biggest concern with LSTM on this project was that the data were simply too small to build a generative representation. No intermediate step produced good rules---steps only differed in their quality of identical replication of the character sequence. The choice of input as 100 character blocks might be increasing this identical text replication pattern. On the other hand, word-by-word prediction will produce an even smaller set of patterns. Stay tuned for the next iteration where a word-by-word LSTM will be examined.\u003C/p>",{"headings":80,"localImagePaths":142,"remoteImagePaths":143,"frontmatter":144,"imagePaths":145},[81,85,88,91,95,99,103,106,109,112,115,118,121,124,127,130,133,136,139],{"depth":82,"slug":83,"text":84},3,"this-post-presents-an-evolving-attempt-to-generate-novel-ferengi-rules-of-acquisition-with-classic-and-contemporary-approaches-to-natural-language-processing","This post presents an evolving attempt to generate novel Ferengi Rules of Acquisition with classic and contemporary approaches to natural language processing.",{"depth":82,"slug":86,"text":87},"the-ferengi-are-a-species-of-uber-capitalists-with-a-1950s-mentality-re-social-issues-that-exist-in-the-star-trek-universe-the-rules-of-acquisition-are-one-part-governmental-charter-one-part-values-system-theyre-written-as-proverbs-so-theyre-naturally-short-and-there-arent-a-ton-of-them-138-in-the-comprehensive-star-trek-universe-in-their-own-words","The Ferengi are a species of uber-capitalists with a 1950’s mentality re: social issues that exist in the Star Trek Universe. The Rules of Acquisition are one part governmental charter, one part values system. They’re written as proverbs, so they’re naturally short and there aren’t a ton of them (~138 in the comprehensive Star Trek universe). In their own words:",{"depth":82,"slug":89,"text":90},"this-means-it-should-be-a-bit-of-a-challenge-to-train-up-an-lstm-to-generate-novel-and-sensible-additions-on-the-other-hand---as-you-might-expect-from-the-socio-cultrual-guidelines-for-a-socially-backwards-profit-at-all-costs-society---the-themes-of-the-rules-are-limited-suggesting-that-a-fairly-vanilla-lstm-might-be-able-to-a-decent-job","This means it should be a bit of a challenge to train up an LSTM to generate novel and sensible additions. On the other hand---as you might expect from the socio-cultrual guidelines for a socially-backwards, profit-at-all-costs society---the themes of the Rules are limited, suggesting that a fairly vanilla LSTM might be able to a decent job.",{"depth":92,"slug":93,"text":94},1,"lstm-on-the-ferengi-rules-of-acquisition","LSTM on the Ferengi Rules of Acquisition",{"depth":96,"slug":97,"text":98},2,"getting-the-rules","getting the rules",{"depth":100,"slug":101,"text":102},4,"load-some-packages","load some packages",{"depth":100,"slug":104,"text":105},"grab-the-text","grab the text",{"depth":100,"slug":107,"text":108},"clean-up-the-text","clean up the text",{"depth":100,"slug":110,"text":111},"build-a-dict-with-all-the-characters","build a dict with all the characters",{"depth":100,"slug":113,"text":114},"basic-text-properties","basic text properties",{"depth":100,"slug":116,"text":117},"advanced-properties--ins-and-outs","advanced properties / ins and outs",{"depth":96,"slug":119,"text":120},"set-up-the-model","set up the model",{"depth":100,"slug":122,"text":123},"create-inputoutput-encodings","create input/output encodings",{"depth":100,"slug":125,"text":126},"set-model-params","set model params",{"depth":100,"slug":128,"text":129},"setup-checkpointing-regime","setup checkpointing regime",{"depth":100,"slug":131,"text":132},"run-the-model","run the model",{"depth":96,"slug":134,"text":135},"checking-out-the-result","checking out the result",{"depth":100,"slug":137,"text":138},"after-200-epochs-we-see-some-progress","after ~200 epochs we see some progress",{"depth":100,"slug":140,"text":141},"at-loss-leq-06-weve-overfit-the-data-closer-inspection-suggests-thats-all-were-doing","at $loss \\leq .06$ we’ve overfit the data; closer inspection suggests that’s all we’re doing",[],[],{"title":71,"excerpt":71,"tags":72,"season":73,"type":9},[],"2017-12-20-roa-lstm.md","2017-11-30-thematic-intrusion-dissertation",{"id":147,"data":149,"filePath":155,"digest":156,"rendered":157,"legacyId":164},{"title":150,"excerpt":151,"tags":152,"season":153,"assets":18,"type":19,"external_url":154},"Causes and Predictors of Thematic Intrusion on Human Similarity Judgments","I successfully defended my dissertation, a behavioral and electrophysiological investigation of human similarity judgments.","taxonomic similarity, thematic association, neuroscience","fall 2017","/pdfs/manuscripts/honke2017final.pdf","src/content/blog/2017-11-30-thematic-intrusion-dissertation.md","34edcc200693393e",{"html":24,"metadata":158},{"headings":159,"localImagePaths":160,"remoteImagePaths":161,"frontmatter":162,"imagePaths":163},[],[],[],{"title":150,"excerpt":151,"tags":152,"season":153,"assets":18,"type":19,"external_url":154},[],"2017-11-30-thematic-intrusion-dissertation.md","2017-2-23-catlearn-diva",{"id":165,"data":167,"filePath":174,"digest":175,"rendered":176,"legacyId":183},{"title":168,"excerpt":169,"tags":170,"season":171,"assets":18,"type":172,"external_url":173},"Catlearn DIVA Package","I wrote a DIVA (DIVergent Autoencoder) model for the Catlearn R library","R, DIVA","spring 2017","code","https://CRAN.R-project.org/package=catlearn","src/content/blog/2017-2-23-catlearn-diva.md","dc9de41eadfb299e",{"html":24,"metadata":177},{"headings":178,"localImagePaths":179,"remoteImagePaths":180,"frontmatter":181,"imagePaths":182},[],[],[],{"title":168,"excerpt":169,"tags":170,"season":171,"assets":18,"type":172,"external_url":173},[],"2017-2-23-catlearn-diva.md","2017-2-24-catlearn-suppls-annouce",{"id":184,"data":186,"filePath":190,"digest":191,"rendered":192,"legacyId":199},{"title":187,"excerpt":188,"tags":170,"season":171,"assets":18,"type":172,"external_url":189},"Catlearn Supplementals Package","Nolan Conaway and I wrote a suite of helper functions to assist with modeling in the Catlearn ecosystem","https://github.com/ghonk/catlearn.suppls","src/content/blog/2017-2-24-catlearn-suppls-annouce.md","4102ea410229e9a1",{"html":24,"metadata":193},{"headings":194,"localImagePaths":195,"remoteImagePaths":196,"frontmatter":197,"imagePaths":198},[],[],[],{"title":187,"excerpt":188,"tags":170,"season":171,"assets":18,"type":172,"external_url":189},[],"2017-2-24-catlearn-suppls-annouce.md","2017-6-28-catlearn-suppls-demo",{"id":200,"data":202,"body":207,"filePath":208,"digest":209,"rendered":210,"legacyId":345},{"title":203,"excerpt":204,"tags":205,"season":206,"type":9},"Getting Started with Catlearn and Catlearn Supplementals","Brief introduction to machine learning and cognitive modeling with Catlearn and Catlearn Supplementals","machine learning, cognitive modeling, catlearn","summer 2017","# background\n\nThis post is intended to help people get started with their own cognitive modelling or machine learning projects in the `catlearn` environment with the help of `catlearn.suppls`, the [Learning and Representation in Cognition Laboratory](http://kurtzlab.psychology.binghamton.edu/)'s suite of helper functions for modeling in `catlearn`.\n\nThe motivation behind `catlearn.suppls` was to create a suite of functions that could be used to rapidly prototype new machine learning architectures that correspond to the cognitive modeling efforts of the LaRC Lab. This includes functions to set up the *state* information of a model and quickly take rectangular data and format it for the `catlearn` design pattern---the stateful list processor.\n\n# getting Started\n\nIf you haven't already done so, download the `catlearn` and `catlearn.suppls` packages.\n\n\n{% highlight r %}\ninstall.packages(c(\"devtools\", \"catlearn\"))\ndevtools::install_github(\"ghonk/catlearn.suppls\")\n{% endhighlight %}\n\n# choosing some data and setting up the model\n\nWe're going to use the classic `iris` dataset and DIVA (the DIVergent Autoencoder) for this demonstration. We need two objects to run a model in `catlearn`, the model's `state` and the training matrix, which we will call `tr`.\n\n\n{% highlight r %}\n# # # load the libraries\nlibrary(catlearn)\nlibrary(catlearn.suppls)\n{% endhighlight %}\n\n*First, we will construct the model's state.* For this demonstration we'll set the hyper-parameters to values that we know---a priori---will play nice with our dataset. For real-world problems, you will likely want optimize these values (see future post on grid search and Bayesian optimization options with `catlearn.suppls`). Detailed description of model hyper-parameters is available in the normal places (e.g., `?slpDIVA`).\n\n\n{% highlight r %}\n# # # setup the inputs, class labels and model state\n# check out our data\nstr(iris)\n{% endhighlight %}\n\n\n\n{% highlight text %}\n## 'data.frame':\t150 obs. of  5 variables:\n##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n##  $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n{% endhighlight %}\n\n\n\n{% highlight r %}\n# find the inputs minus the labels\nins \u003C- iris[,colnames(iris) != 'Species']\n\n# create a separate vector for the labels (labels must be numeric)\nlabs \u003C- as.numeric(iris[,'Species'])\n\n# get number of categories and features\nnfeats \u003C- dim(ins)[2]\nncats \u003C- length(unique(labs))\n\n# construct a state list\nst \u003C- list(learning_rate = 0.15, num_feats = nfeats, num_hids = 6, num_cats = ncats,\n  beta_val = 0, phi = 1, continuous = TRUE, in_wts = NULL, out_wts = NULL, wts_range = 1,\n  wts_center = 0, colskip = 4)\n{% endhighlight %}\n\nWe can then use `catleanr.suppls` to create our training matrix\n\n\n{% highlight r %}\n# tr_init initializes an empty training matrix\ntr \u003C- tr_init(nfeats, ncats)\n\n# tr_add fills in the data and procedure (i.e., training, test, model reset)\ntr \u003C- tr_add(inputs = ins, tr = tr, labels = labs, blocks = 12, ctrl = 0, \n  shuffle = TRUE, reset = TRUE)\n{% endhighlight %}\n\nHere's what our training matrix looks like after the setup procedure:\n\n{% highlight r %}\nhead(tr)\n{% endhighlight %}\n\n\n\n{% highlight text %}\n##      ctrl trial blk example  x1  x2  x3  x4 t1 t2 t3\n## [1,]    1     1   1     118 7.7 3.8 6.7 2.2 -1 -1  1\n## [2,]    0     2   1      64 6.1 2.9 4.7 1.4 -1  1 -1\n## [3,]    0     3   1      23 4.6 3.6 1.0 0.2  1 -1 -1\n## [4,]    0     4   1      87 6.7 3.1 4.7 1.5 -1  1 -1\n## [5,]    0     5   1      99 5.1 2.5 3.0 1.1 -1  1 -1\n## [6,]    0     6   1      92 6.1 3.0 4.6 1.4 -1  1 -1\n{% endhighlight %}\n\nFinally, we run the model with our state list `st` and training matrix `tr`\n\n\n{% highlight r %}\ndiva_model \u003C- slpDIVA(st, tr)\n{% endhighlight %}\n\nWe can examine performance of the model easily. \n\n\n{% highlight r %}\n# # # use response_probs to extract the response probabilities \n# # # for the target categories (for every training step (trial) \n# # # or averaged across blocks)\n\nresponse_probs(tr, diva_model$out, blocks = TRUE)\n{% endhighlight %}\n\n\n\n{% highlight text %}\n##  [1] 0.7852448 0.8431547 0.8103888 0.8067568 0.8087171 0.8102305 0.8135465\n##  [8] 0.8028162 0.8466575 0.8375568 0.8025279 0.8350697\n{% endhighlight %}\n\n`plot_training` is a simple function used to plot the learning of one or more models.\n\n\n{% highlight r %}\nplot_training(list(response_probs(tr, diva_model$out, blocks = TRUE)))\n{% endhighlight %}\n\n![plot of chunk unnamed-chunk-9](/assets/rfigs/unnamed-chunk-9-1.svg)\n\nSo with no optimization, we can see that DIVA learns about as much as it is going to learn after one pass through our 150 item training set (obviously absent any cross-validation). Where does it go wrong? You might like to examine which items are not being correctly classified---you can do so by combining the classification probabilities with the original training matrix.\n\n\n{% highlight r %}\n# # # if we want to look at individual classication decisions, \n# # # we can do so by combining the model's output with the \n# # # original training matrix\n\ntrn_result \u003C- cbind(tr, round(diva_model$out, 4))\ntail(trn_result)\n{% endhighlight %}\n\n\n\n{% highlight text %}\n##         ctrl trial blk example  x1  x2  x3  x4 t1 t2 t3                     \n## [1795,]    0  1795  12     123 7.7 2.8 6.7 2.0 -1 -1  1 0.0005 0.0006 0.9989\n## [1796,]    0  1796  12      82 5.5 2.4 3.7 1.0 -1  1 -1 0.0000 0.9999 0.0000\n## [1797,]    0  1797  12     125 6.7 3.3 5.7 2.1 -1 -1  1 0.1204 0.1765 0.7030\n## [1798,]    0  1798  12       1 5.1 3.5 1.4 0.2  1 -1 -1 1.0000 0.0000 0.0000\n## [1799,]    0  1799  12     144 6.8 3.2 5.9 2.3 -1 -1  1 0.0000 0.0000 1.0000\n## [1800,]    0  1800  12     129 6.4 2.8 5.6 2.1 -1 -1  1 0.0001 0.0002 0.9996\n{% endhighlight %}\n\nYou might also like to see how a number of initializations do on the problem. It's good practice to average over a series of initializations---something we did not do in this demonstration.\n\n\n{% highlight r %}\n# # # Run 5 models with the same params on the same training set\nmodel_inits \u003C- lapply(1:5, function(x) slpDIVA(st, tr))\n\n# # # determine the response probability for the correct class\nmodel_resp_probs \u003C- \n  lapply(model_inits, function(x) {\n    response_probs(tr, x$out, blocks = TRUE)})\n\n# # # plot the leanrning curves\nplot_training(model_resp_probs)\n{% endhighlight %}\n\n![plot of chunk unnamed-chunk-11](/assets/rfigs/unnamed-chunk-11-1.svg)\n\nHere we see that there is a fair amount of variation across initializations. This suggests it would be smart to follow the typical procedure of averaging across a series of models to accurately represent the response probabilities. It also suggests that our approach would likely benefit from some optimization and validation. \n\n\nFuture demos will explore the tools within `catlearn.suppls` used to optimize hyper-parameters and examine the hidden unit representation space toward the goal of uncovering new insight about the problem.","src/content/blog/2017-6-28-catlearn-suppls-demo.md","a90c487be2a3c45c",{"html":211,"metadata":212},"\u003Ch1 id=\"background\">background\u003C/h1>\n\u003Cp>This post is intended to help people get started with their own cognitive modelling or machine learning projects in the \u003Ccode>catlearn\u003C/code> environment with the help of \u003Ccode>catlearn.suppls\u003C/code>, the \u003Ca href=\"http://kurtzlab.psychology.binghamton.edu/\">Learning and Representation in Cognition Laboratory\u003C/a>’s suite of helper functions for modeling in \u003Ccode>catlearn\u003C/code>.\u003C/p>\n\u003Cp>The motivation behind \u003Ccode>catlearn.suppls\u003C/code> was to create a suite of functions that could be used to rapidly prototype new machine learning architectures that correspond to the cognitive modeling efforts of the LaRC Lab. This includes functions to set up the \u003Cem>state\u003C/em> information of a model and quickly take rectangular data and format it for the \u003Ccode>catlearn\u003C/code> design pattern---the stateful list processor.\u003C/p>\n\u003Ch1 id=\"getting-started\">getting Started\u003C/h1>\n\u003Cp>If you haven’t already done so, download the \u003Ccode>catlearn\u003C/code> and \u003Ccode>catlearn.suppls\u003C/code> packages.\u003C/p>\n\u003Cp>{% highlight r %}\ninstall.packages(c(“devtools”, “catlearn”))\ndevtools::install_github(“ghonk/catlearn.suppls”)\n{% endhighlight %}\u003C/p>\n\u003Ch1 id=\"choosing-some-data-and-setting-up-the-model\">choosing some data and setting up the model\u003C/h1>\n\u003Cp>We’re going to use the classic \u003Ccode>iris\u003C/code> dataset and DIVA (the DIVergent Autoencoder) for this demonstration. We need two objects to run a model in \u003Ccode>catlearn\u003C/code>, the model’s \u003Ccode>state\u003C/code> and the training matrix, which we will call \u003Ccode>tr\u003C/code>.\u003C/p>\n\u003Cp>{% highlight r %}\u003C/p>\n\u003Ch1 id=\"--load-the-libraries\"># # load the libraries\u003C/h1>\n\u003Cp>library(catlearn)\nlibrary(catlearn.suppls)\n{% endhighlight %}\u003C/p>\n\u003Cp>\u003Cem>First, we will construct the model’s state.\u003C/em> For this demonstration we’ll set the hyper-parameters to values that we know---a priori---will play nice with our dataset. For real-world problems, you will likely want optimize these values (see future post on grid search and Bayesian optimization options with \u003Ccode>catlearn.suppls\u003C/code>). Detailed description of model hyper-parameters is available in the normal places (e.g., \u003Ccode>?slpDIVA\u003C/code>).\u003C/p>\n\u003Cp>{% highlight r %}\u003C/p>\n\u003Ch1 id=\"--setup-the-inputs-class-labels-and-model-state\"># # setup the inputs, class labels and model state\u003C/h1>\n\u003Ch1 id=\"check-out-our-data\">check out our data\u003C/h1>\n\u003Cp>str(iris)\n{% endhighlight %}\u003C/p>\n\u003Cp>{% highlight text %}\u003C/p>\n\u003Ch2 id=\"dataframe150-obs-of--5-variables\">‘data.frame’:\t150 obs. of  5 variables:\u003C/h2>\n\u003Ch2 id=\"-sepallength-num--51-49-47-46-5-54-46-5-44-49\">$ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 …\u003C/h2>\n\u003Ch2 id=\"-sepalwidth--num--35-3-32-31-36-39-34-34-29-31\">$ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 …\u003C/h2>\n\u003Ch2 id=\"-petallength-num--14-14-13-15-14-17-14-15-14-15\">$ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 …\u003C/h2>\n\u003Ch2 id=\"-petalwidth--num--02-02-02-02-02-04-03-02-02-01\">$ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 …\u003C/h2>\n\u003Ch2 id=\"-species------factor-w-3-levels-setosaversicolor-1-1-1-1-1-1-1-1-1-1\">$ Species     : Factor w/ 3 levels “setosa”,“versicolor”,..: 1 1 1 1 1 1 1 1 1 1 …\u003C/h2>\n\u003Cp>{% endhighlight %}\u003C/p>\n\u003Cp>{% highlight r %}\u003C/p>\n\u003Ch1 id=\"find-the-inputs-minus-the-labels\">find the inputs minus the labels\u003C/h1>\n\u003Cp>ins &#x3C;- iris[,colnames(iris) != ‘Species’]\u003C/p>\n\u003Ch1 id=\"create-a-separate-vector-for-the-labels-labels-must-be-numeric\">create a separate vector for the labels (labels must be numeric)\u003C/h1>\n\u003Cp>labs &#x3C;- as.numeric(iris[,‘Species’])\u003C/p>\n\u003Ch1 id=\"get-number-of-categories-and-features\">get number of categories and features\u003C/h1>\n\u003Cp>nfeats &#x3C;- dim(ins)[2]\nncats &#x3C;- length(unique(labs))\u003C/p>\n\u003Ch1 id=\"construct-a-state-list\">construct a state list\u003C/h1>\n\u003Cp>st &#x3C;- list(learning_rate = 0.15, num_feats = nfeats, num_hids = 6, num_cats = ncats,\nbeta_val = 0, phi = 1, continuous = TRUE, in_wts = NULL, out_wts = NULL, wts_range = 1,\nwts_center = 0, colskip = 4)\n{% endhighlight %}\u003C/p>\n\u003Cp>We can then use \u003Ccode>catleanr.suppls\u003C/code> to create our training matrix\u003C/p>\n\u003Cp>{% highlight r %}\u003C/p>\n\u003Ch1 id=\"tr_init-initializes-an-empty-training-matrix\">tr_init initializes an empty training matrix\u003C/h1>\n\u003Cp>tr &#x3C;- tr_init(nfeats, ncats)\u003C/p>\n\u003Ch1 id=\"tr_add-fills-in-the-data-and-procedure-ie-training-test-model-reset\">tr_add fills in the data and procedure (i.e., training, test, model reset)\u003C/h1>\n\u003Cp>tr &#x3C;- tr_add(inputs = ins, tr = tr, labels = labs, blocks = 12, ctrl = 0,\nshuffle = TRUE, reset = TRUE)\n{% endhighlight %}\u003C/p>\n\u003Cp>Here’s what our training matrix looks like after the setup procedure:\u003C/p>\n\u003Cp>{% highlight r %}\nhead(tr)\n{% endhighlight %}\u003C/p>\n\u003Cp>{% highlight text %}\u003C/p>\n\u003Ch2 id=\"ctrl-trial-blk-example--x1--x2--x3--x4-t1-t2-t3\">ctrl trial blk example  x1  x2  x3  x4 t1 t2 t3\u003C/h2>\n\u003Ch2 id=\"1----1-----1---1-----118-77-38-67-22--1--1--1\">[1,]    1     1   1     118 7.7 3.8 6.7 2.2 -1 -1  1\u003C/h2>\n\u003Ch2 id=\"2----0-----2---1------64-61-29-47-14--1--1--1\">[2,]    0     2   1      64 6.1 2.9 4.7 1.4 -1  1 -1\u003C/h2>\n\u003Ch2 id=\"3----0-----3---1------23-46-36-10-02--1--1--1\">[3,]    0     3   1      23 4.6 3.6 1.0 0.2  1 -1 -1\u003C/h2>\n\u003Ch2 id=\"4----0-----4---1------87-67-31-47-15--1--1--1\">[4,]    0     4   1      87 6.7 3.1 4.7 1.5 -1  1 -1\u003C/h2>\n\u003Ch2 id=\"5----0-----5---1------99-51-25-30-11--1--1--1\">[5,]    0     5   1      99 5.1 2.5 3.0 1.1 -1  1 -1\u003C/h2>\n\u003Ch2 id=\"6----0-----6---1------92-61-30-46-14--1--1--1\">[6,]    0     6   1      92 6.1 3.0 4.6 1.4 -1  1 -1\u003C/h2>\n\u003Cp>{% endhighlight %}\u003C/p>\n\u003Cp>Finally, we run the model with our state list \u003Ccode>st\u003C/code> and training matrix \u003Ccode>tr\u003C/code>\u003C/p>\n\u003Cp>{% highlight r %}\ndiva_model &#x3C;- slpDIVA(st, tr)\n{% endhighlight %}\u003C/p>\n\u003Cp>We can examine performance of the model easily.\u003C/p>\n\u003Cp>{% highlight r %}\u003C/p>\n\u003Ch1 id=\"--use-response_probs-to-extract-the-response-probabilities\"># # use response_probs to extract the response probabilities\u003C/h1>\n\u003Ch1 id=\"--for-the-target-categories-for-every-training-step-trial\"># # for the target categories (for every training step (trial)\u003C/h1>\n\u003Ch1 id=\"--or-averaged-across-blocks\"># # or averaged across blocks)\u003C/h1>\n\u003Cp>response_probs(tr, diva_model$out, blocks = TRUE)\n{% endhighlight %}\u003C/p>\n\u003Cp>{% highlight text %}\u003C/p>\n\u003Ch2 id=\"1-07852448-08431547-08103888-08067568-08087171-08102305-08135465\">[1] 0.7852448 0.8431547 0.8103888 0.8067568 0.8087171 0.8102305 0.8135465\u003C/h2>\n\u003Ch2 id=\"8-08028162-08466575-08375568-08025279-08350697\">[8] 0.8028162 0.8466575 0.8375568 0.8025279 0.8350697\u003C/h2>\n\u003Cp>{% endhighlight %}\u003C/p>\n\u003Cp>\u003Ccode>plot_training\u003C/code> is a simple function used to plot the learning of one or more models.\u003C/p>\n\u003Cp>{% highlight r %}\nplot_training(list(response_probs(tr, diva_model$out, blocks = TRUE)))\n{% endhighlight %}\u003C/p>\n\u003Cp>\u003Cimg src=\"/assets/rfigs/unnamed-chunk-9-1.svg\" alt=\"plot of chunk unnamed-chunk-9\">\u003C/p>\n\u003Cp>So with no optimization, we can see that DIVA learns about as much as it is going to learn after one pass through our 150 item training set (obviously absent any cross-validation). Where does it go wrong? You might like to examine which items are not being correctly classified---you can do so by combining the classification probabilities with the original training matrix.\u003C/p>\n\u003Cp>{% highlight r %}\u003C/p>\n\u003Ch1 id=\"--if-we-want-to-look-at-individual-classication-decisions\"># # if we want to look at individual classication decisions,\u003C/h1>\n\u003Ch1 id=\"--we-can-do-so-by-combining-the-models-output-with-the\"># # we can do so by combining the model’s output with the\u003C/h1>\n\u003Ch1 id=\"--original-training-matrix\"># # original training matrix\u003C/h1>\n\u003Cp>trn_result &#x3C;- cbind(tr, round(diva_model$out, 4))\ntail(trn_result)\n{% endhighlight %}\u003C/p>\n\u003Cp>{% highlight text %}\u003C/p>\n\u003Ch2 id=\"ctrl-trial-blk-example--x1--x2--x3--x4-t1-t2-t3-1\">ctrl trial blk example  x1  x2  x3  x4 t1 t2 t3\u003C/h2>\n\u003Ch2 id=\"1795----0--1795--12-----123-77-28-67-20--1--1--1-00005-00006-09989\">[1795,]    0  1795  12     123 7.7 2.8 6.7 2.0 -1 -1  1 0.0005 0.0006 0.9989\u003C/h2>\n\u003Ch2 id=\"1796----0--1796--12------82-55-24-37-10--1--1--1-00000-09999-00000\">[1796,]    0  1796  12      82 5.5 2.4 3.7 1.0 -1  1 -1 0.0000 0.9999 0.0000\u003C/h2>\n\u003Ch2 id=\"1797----0--1797--12-----125-67-33-57-21--1--1--1-01204-01765-07030\">[1797,]    0  1797  12     125 6.7 3.3 5.7 2.1 -1 -1  1 0.1204 0.1765 0.7030\u003C/h2>\n\u003Ch2 id=\"1798----0--1798--12-------1-51-35-14-02--1--1--1-10000-00000-00000\">[1798,]    0  1798  12       1 5.1 3.5 1.4 0.2  1 -1 -1 1.0000 0.0000 0.0000\u003C/h2>\n\u003Ch2 id=\"1799----0--1799--12-----144-68-32-59-23--1--1--1-00000-00000-10000\">[1799,]    0  1799  12     144 6.8 3.2 5.9 2.3 -1 -1  1 0.0000 0.0000 1.0000\u003C/h2>\n\u003Ch2 id=\"1800----0--1800--12-----129-64-28-56-21--1--1--1-00001-00002-09996\">[1800,]    0  1800  12     129 6.4 2.8 5.6 2.1 -1 -1  1 0.0001 0.0002 0.9996\u003C/h2>\n\u003Cp>{% endhighlight %}\u003C/p>\n\u003Cp>You might also like to see how a number of initializations do on the problem. It’s good practice to average over a series of initializations---something we did not do in this demonstration.\u003C/p>\n\u003Cp>{% highlight r %}\u003C/p>\n\u003Ch1 id=\"--run-5-models-with-the-same-params-on-the-same-training-set\"># # Run 5 models with the same params on the same training set\u003C/h1>\n\u003Cp>model_inits &#x3C;- lapply(1:5, function(x) slpDIVA(st, tr))\u003C/p>\n\u003Ch1 id=\"--determine-the-response-probability-for-the-correct-class\"># # determine the response probability for the correct class\u003C/h1>\n\u003Cp>model_resp_probs &#x3C;-\nlapply(model_inits, function(x) {\nresponse_probs(tr, x$out, blocks = TRUE)})\u003C/p>\n\u003Ch1 id=\"--plot-the-leanrning-curves\"># # plot the leanrning curves\u003C/h1>\n\u003Cp>plot_training(model_resp_probs)\n{% endhighlight %}\u003C/p>\n\u003Cp>\u003Cimg src=\"/assets/rfigs/unnamed-chunk-11-1.svg\" alt=\"plot of chunk unnamed-chunk-11\">\u003C/p>\n\u003Cp>Here we see that there is a fair amount of variation across initializations. This suggests it would be smart to follow the typical procedure of averaging across a series of models to accurately represent the response probabilities. It also suggests that our approach would likely benefit from some optimization and validation.\u003C/p>\n\u003Cp>Future demos will explore the tools within \u003Ccode>catlearn.suppls\u003C/code> used to optimize hyper-parameters and examine the hidden unit representation space toward the goal of uncovering new insight about the problem.\u003C/p>",{"headings":213,"localImagePaths":341,"remoteImagePaths":342,"frontmatter":343,"imagePaths":344},[214,216,219,222,225,228,231,234,237,240,243,246,249,252,255,258,261,264,267,270,273,276,279,282,285,288,291,294,297,300,303,306,309,312,314,317,320,323,326,329,332,335,338],{"depth":92,"slug":215,"text":215},"background",{"depth":92,"slug":217,"text":218},"getting-started","getting Started",{"depth":92,"slug":220,"text":221},"choosing-some-data-and-setting-up-the-model","choosing some data and setting up the model",{"depth":92,"slug":223,"text":224},"--load-the-libraries","# # load the libraries",{"depth":92,"slug":226,"text":227},"--setup-the-inputs-class-labels-and-model-state","# # setup the inputs, class labels and model state",{"depth":92,"slug":229,"text":230},"check-out-our-data","check out our data",{"depth":96,"slug":232,"text":233},"dataframe150-obs-of--5-variables","‘data.frame’:\t150 obs. of  5 variables:",{"depth":96,"slug":235,"text":236},"-sepallength-num--51-49-47-46-5-54-46-5-44-49","$ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 …",{"depth":96,"slug":238,"text":239},"-sepalwidth--num--35-3-32-31-36-39-34-34-29-31","$ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 …",{"depth":96,"slug":241,"text":242},"-petallength-num--14-14-13-15-14-17-14-15-14-15","$ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 …",{"depth":96,"slug":244,"text":245},"-petalwidth--num--02-02-02-02-02-04-03-02-02-01","$ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 …",{"depth":96,"slug":247,"text":248},"-species------factor-w-3-levels-setosaversicolor-1-1-1-1-1-1-1-1-1-1","$ Species     : Factor w/ 3 levels “setosa”,“versicolor”,..: 1 1 1 1 1 1 1 1 1 1 …",{"depth":92,"slug":250,"text":251},"find-the-inputs-minus-the-labels","find the inputs minus the labels",{"depth":92,"slug":253,"text":254},"create-a-separate-vector-for-the-labels-labels-must-be-numeric","create a separate vector for the labels (labels must be numeric)",{"depth":92,"slug":256,"text":257},"get-number-of-categories-and-features","get number of categories and features",{"depth":92,"slug":259,"text":260},"construct-a-state-list","construct a state list",{"depth":92,"slug":262,"text":263},"tr_init-initializes-an-empty-training-matrix","tr_init initializes an empty training matrix",{"depth":92,"slug":265,"text":266},"tr_add-fills-in-the-data-and-procedure-ie-training-test-model-reset","tr_add fills in the data and procedure (i.e., training, test, model reset)",{"depth":96,"slug":268,"text":269},"ctrl-trial-blk-example--x1--x2--x3--x4-t1-t2-t3","ctrl trial blk example  x1  x2  x3  x4 t1 t2 t3",{"depth":96,"slug":271,"text":272},"1----1-----1---1-----118-77-38-67-22--1--1--1","[1,]    1     1   1     118 7.7 3.8 6.7 2.2 -1 -1  1",{"depth":96,"slug":274,"text":275},"2----0-----2---1------64-61-29-47-14--1--1--1","[2,]    0     2   1      64 6.1 2.9 4.7 1.4 -1  1 -1",{"depth":96,"slug":277,"text":278},"3----0-----3---1------23-46-36-10-02--1--1--1","[3,]    0     3   1      23 4.6 3.6 1.0 0.2  1 -1 -1",{"depth":96,"slug":280,"text":281},"4----0-----4---1------87-67-31-47-15--1--1--1","[4,]    0     4   1      87 6.7 3.1 4.7 1.5 -1  1 -1",{"depth":96,"slug":283,"text":284},"5----0-----5---1------99-51-25-30-11--1--1--1","[5,]    0     5   1      99 5.1 2.5 3.0 1.1 -1  1 -1",{"depth":96,"slug":286,"text":287},"6----0-----6---1------92-61-30-46-14--1--1--1","[6,]    0     6   1      92 6.1 3.0 4.6 1.4 -1  1 -1",{"depth":92,"slug":289,"text":290},"--use-response_probs-to-extract-the-response-probabilities","# # use response_probs to extract the response probabilities",{"depth":92,"slug":292,"text":293},"--for-the-target-categories-for-every-training-step-trial","# # for the target categories (for every training step (trial)",{"depth":92,"slug":295,"text":296},"--or-averaged-across-blocks","# # or averaged across blocks)",{"depth":96,"slug":298,"text":299},"1-07852448-08431547-08103888-08067568-08087171-08102305-08135465","[1] 0.7852448 0.8431547 0.8103888 0.8067568 0.8087171 0.8102305 0.8135465",{"depth":96,"slug":301,"text":302},"8-08028162-08466575-08375568-08025279-08350697","[8] 0.8028162 0.8466575 0.8375568 0.8025279 0.8350697",{"depth":92,"slug":304,"text":305},"--if-we-want-to-look-at-individual-classication-decisions","# # if we want to look at individual classication decisions,",{"depth":92,"slug":307,"text":308},"--we-can-do-so-by-combining-the-models-output-with-the","# # we can do so by combining the model’s output with the",{"depth":92,"slug":310,"text":311},"--original-training-matrix","# # original training matrix",{"depth":96,"slug":313,"text":269},"ctrl-trial-blk-example--x1--x2--x3--x4-t1-t2-t3-1",{"depth":96,"slug":315,"text":316},"1795----0--1795--12-----123-77-28-67-20--1--1--1-00005-00006-09989","[1795,]    0  1795  12     123 7.7 2.8 6.7 2.0 -1 -1  1 0.0005 0.0006 0.9989",{"depth":96,"slug":318,"text":319},"1796----0--1796--12------82-55-24-37-10--1--1--1-00000-09999-00000","[1796,]    0  1796  12      82 5.5 2.4 3.7 1.0 -1  1 -1 0.0000 0.9999 0.0000",{"depth":96,"slug":321,"text":322},"1797----0--1797--12-----125-67-33-57-21--1--1--1-01204-01765-07030","[1797,]    0  1797  12     125 6.7 3.3 5.7 2.1 -1 -1  1 0.1204 0.1765 0.7030",{"depth":96,"slug":324,"text":325},"1798----0--1798--12-------1-51-35-14-02--1--1--1-10000-00000-00000","[1798,]    0  1798  12       1 5.1 3.5 1.4 0.2  1 -1 -1 1.0000 0.0000 0.0000",{"depth":96,"slug":327,"text":328},"1799----0--1799--12-----144-68-32-59-23--1--1--1-00000-00000-10000","[1799,]    0  1799  12     144 6.8 3.2 5.9 2.3 -1 -1  1 0.0000 0.0000 1.0000",{"depth":96,"slug":330,"text":331},"1800----0--1800--12-----129-64-28-56-21--1--1--1-00001-00002-09996","[1800,]    0  1800  12     129 6.4 2.8 5.6 2.1 -1 -1  1 0.0001 0.0002 0.9996",{"depth":92,"slug":333,"text":334},"--run-5-models-with-the-same-params-on-the-same-training-set","# # Run 5 models with the same params on the same training set",{"depth":92,"slug":336,"text":337},"--determine-the-response-probability-for-the-correct-class","# # determine the response probability for the correct class",{"depth":92,"slug":339,"text":340},"--plot-the-leanrning-curves","# # plot the leanrning curves",[],[],{"title":203,"excerpt":204,"tags":205,"season":206,"type":9},[],"2017-6-28-catlearn-suppls-demo.md","2017-7-30-catlearn-grid-search",{"id":346,"data":348,"body":352,"filePath":353,"digest":354,"rendered":355,"legacyId":454},{"title":349,"excerpt":350,"tags":351,"season":206,"type":9},"Grid Search with Catlearn and Catlearn Supplementals","Quick tutorial on grid search with Catlearn and Catlearn Supplementals","ml, datascience, DIVA","This is a brief demonstration of the grid search helper functions provided in `catlearn.suppls`. There are both parallelized and non-parallelized versions. \n\nload libraries \n\n\n{% highlight r %}\nlibrary(catlearn)\nlibrary(catlearn.suppls)\n{% endhighlight %}\n\n*initialize variables.* Create a named list for each hyperparameter you plan to test in the grid search, specify how many random model initializations you'd like to average across to calculate response probabilities for each parameter combination.  \n\n\n{% highlight r %}\n# # parameter list\nshort_param_list \u003C- list(beta_val = c(0, 1, 2, 3),\n                    learning_rate = c(.05, .10, .15),\n                         num_hids = c(4, 5, 6))\n\n# long_param_list \u003C- list(beta_val = c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10),\n#                    learning_rate = c(.05, .10, .15, .20, .25, .35),\n#                         num_hids = c(4, 5, 6, 10, 20))\n\n# # # number of initializations\nnum_inits = 4\n\n# # # data\n# input_list \u003C- get_test_inputs('type1')\ninput_list \u003C- get_test_inputs('type4')\n\n# # # fit type\nfit_type \u003C- 'bestacc'\n\n# # # fit vector\ncrit_fit_vector \u003C- NULL\n{% endhighlight %}\n\nrun it\n\n\n{% highlight r %}\n# # # single core\nsystem.time(gs_output \u003C- diva_grid_search(short_param_list, num_inits, input_list))\n{% endhighlight %}\n\n\n\n{% highlight text %}\n##    user  system elapsed \n##    3.95    0.00    3.99\n{% endhighlight %}\n\n\n\n{% highlight r %}\n# # # parallelized\nsystem.time(gs_output \u003C- diva_grid_search_par(short_param_list, num_inits, input_list))\n{% endhighlight %}\n\n\n\n{% highlight text %}\n##    user  system elapsed \n##    0.03    0.00    3.04\n{% endhighlight %}\n\nexamine the results with `plot_training`\n\n\n{% highlight r %}\nplot_training(lapply(gs_output, function(x) x$resp_probs))\n{% endhighlight %}\n\n![plot of chunk unnamed-chunk-16](/assets/rfigs/unnamed-chunk-16-1.svg)\n\nexamine the detailed results of a grid search run\n\n\n{% highlight r %}\n# # # how many paramter settings did we have?\n(n_models \u003C- length(gs_output))\n{% endhighlight %}\n\n\n\n{% highlight text %}\n## [1] 36\n{% endhighlight %}\n\n\n\n{% highlight r %}\n# # # what was the accuracy distribution?\nfinal_accuracy \u003C- lapply(gs_output, function(x) {x$resp_probs[12]})\nplot(1:n_models, final_accuracy)\n{% endhighlight %}\n\n![plot of chunk unnamed-chunk-17](/assets/rfigs/unnamed-chunk-17-1.svg)\n\n{% highlight r %}\n# # # what parameter setting had the best performance?\ngs_output[[which.max(final_accuracy)]]$params\n{% endhighlight %}\n\n\n\n{% highlight text %}\n##    beta_val learning_rate num_hids\n## 33        0          0.15        6\n{% endhighlight %}\n\n\n\n{% highlight r %}\n# # # what parameter setting had the worst perfomance?\ngs_output[[which.min(final_accuracy)]]$params\n{% endhighlight %}\n\n\n\n{% highlight text %}\n##   beta_val learning_rate num_hids\n## 7        2           0.1        4\n{% endhighlight %}\n\n\n\n{% highlight r %}\n# # # plot em\nplot_training(list(gs_output[[which.max(final_accuracy)]]$resp_probs, \n  gs_output[[which.min(final_accuracy)]]$resp_probs))\n{% endhighlight %}\n\n![plot of chunk unnamed-chunk-17](/assets/rfigs/unnamed-chunk-17-2.svg)\n\n{% highlight r %}\n# # # what comes as output for each parameter setting?\nnames(gs_output[[which.max(final_accuracy)]])\n{% endhighlight %}\n\n\n\n{% highlight text %}\n## [1] \"resp_probs\" \"params\"     \"st\"\n{% endhighlight %}\n\n\n\n{% highlight r %}\n# # # plot the training curves for a parameter subset (hid units = 5)\nhidunit5_respprobs \u003C- list()\nfor (i in 1:length(gs_output)) {\n  if (gs_output[[i]]$params$num_hids == 5){\n    hidunit5_respprobs[[paste0(i)]] \u003C- gs_output[[i]]$resp_probs\n  } \n}\n\n# # # how many?\n(n_models \u003C- length(hidunit5_respprobs))\n{% endhighlight %}\n\n\n\n{% highlight text %}\n## [1] 12\n{% endhighlight %}\n\n\n\n{% highlight r %}\n# # # accuracy?\nplot(1:n_models, unlist(lapply(hidunit5_respprobs, function(x) x[[12]])))\n{% endhighlight %}\n\n![plot of chunk unnamed-chunk-17](/assets/rfigs/unnamed-chunk-17-3.svg)\n\n{% highlight r %}\nplot_training(hidunit5_respprobs)\n{% endhighlight %}\n\n![plot of chunk unnamed-chunk-17](/assets/rfigs/unnamed-chunk-17-4.svg)","src/content/blog/2017-7-30-catlearn-grid-search.md","c287227731d975bf",{"html":356,"metadata":357},"\u003Cp>This is a brief demonstration of the grid search helper functions provided in \u003Ccode>catlearn.suppls\u003C/code>. There are both parallelized and non-parallelized versions.\u003C/p>\n\u003Cp>load libraries\u003C/p>\n\u003Cp>{% highlight r %}\nlibrary(catlearn)\nlibrary(catlearn.suppls)\n{% endhighlight %}\u003C/p>\n\u003Cp>\u003Cem>initialize variables.\u003C/em> Create a named list for each hyperparameter you plan to test in the grid search, specify how many random model initializations you’d like to average across to calculate response probabilities for each parameter combination.\u003C/p>\n\u003Cp>{% highlight r %}\u003C/p>\n\u003Ch1 id=\"-parameter-list\"># parameter list\u003C/h1>\n\u003Cp>short_param_list &#x3C;- list(beta_val = c(0, 1, 2, 3),\nlearning_rate = c(.05, .10, .15),\nnum_hids = c(4, 5, 6))\u003C/p>\n\u003Ch1 id=\"long_param_list---listbeta_val--c0-1-2-3-4-5-6-7-8-9-10\">long_param_list &#x3C;- list(beta_val = c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10),\u003C/h1>\n\u003Ch1 id=\"learning_rate--c05-10-15-20-25-35\">learning_rate = c(.05, .10, .15, .20, .25, .35),\u003C/h1>\n\u003Ch1 id=\"num_hids--c4-5-6-10-20\">num_hids = c(4, 5, 6, 10, 20))\u003C/h1>\n\u003Ch1 id=\"--number-of-initializations\"># # number of initializations\u003C/h1>\n\u003Cp>num_inits = 4\u003C/p>\n\u003Ch1 id=\"--data\"># # data\u003C/h1>\n\u003Ch1 id=\"input_list---get_test_inputstype1\">input_list &#x3C;- get_test_inputs(‘type1’)\u003C/h1>\n\u003Cp>input_list &#x3C;- get_test_inputs(‘type4’)\u003C/p>\n\u003Ch1 id=\"--fit-type\"># # fit type\u003C/h1>\n\u003Cp>fit_type &#x3C;- ‘bestacc’\u003C/p>\n\u003Ch1 id=\"--fit-vector\"># # fit vector\u003C/h1>\n\u003Cp>crit_fit_vector &#x3C;- NULL\n{% endhighlight %}\u003C/p>\n\u003Cp>run it\u003C/p>\n\u003Cp>{% highlight r %}\u003C/p>\n\u003Ch1 id=\"--single-core\"># # single core\u003C/h1>\n\u003Cp>system.time(gs_output &#x3C;- diva_grid_search(short_param_list, num_inits, input_list))\n{% endhighlight %}\u003C/p>\n\u003Cp>{% highlight text %}\u003C/p>\n\u003Ch2 id=\"user--system-elapsed\">user  system elapsed\u003C/h2>\n\u003Ch2 id=\"395----000----399\">3.95    0.00    3.99\u003C/h2>\n\u003Cp>{% endhighlight %}\u003C/p>\n\u003Cp>{% highlight r %}\u003C/p>\n\u003Ch1 id=\"--parallelized\"># # parallelized\u003C/h1>\n\u003Cp>system.time(gs_output &#x3C;- diva_grid_search_par(short_param_list, num_inits, input_list))\n{% endhighlight %}\u003C/p>\n\u003Cp>{% highlight text %}\u003C/p>\n\u003Ch2 id=\"user--system-elapsed-1\">user  system elapsed\u003C/h2>\n\u003Ch2 id=\"003----000----304\">0.03    0.00    3.04\u003C/h2>\n\u003Cp>{% endhighlight %}\u003C/p>\n\u003Cp>examine the results with \u003Ccode>plot_training\u003C/code>\u003C/p>\n\u003Cp>{% highlight r %}\nplot_training(lapply(gs_output, function(x) x$resp_probs))\n{% endhighlight %}\u003C/p>\n\u003Cp>\u003Cimg src=\"/assets/rfigs/unnamed-chunk-16-1.svg\" alt=\"plot of chunk unnamed-chunk-16\">\u003C/p>\n\u003Cp>examine the detailed results of a grid search run\u003C/p>\n\u003Cp>{% highlight r %}\u003C/p>\n\u003Ch1 id=\"--how-many-paramter-settings-did-we-have\"># # how many paramter settings did we have?\u003C/h1>\n\u003Cp>(n_models &#x3C;- length(gs_output))\n{% endhighlight %}\u003C/p>\n\u003Cp>{% highlight text %}\u003C/p>\n\u003Ch2 id=\"1-36\">[1] 36\u003C/h2>\n\u003Cp>{% endhighlight %}\u003C/p>\n\u003Cp>{% highlight r %}\u003C/p>\n\u003Ch1 id=\"--what-was-the-accuracy-distribution\"># # what was the accuracy distribution?\u003C/h1>\n\u003Cp>final_accuracy &#x3C;- lapply(gs_output, function(x) {x$resp_probs[12]})\nplot(1:n_models, final_accuracy)\n{% endhighlight %}\u003C/p>\n\u003Cp>\u003Cimg src=\"/assets/rfigs/unnamed-chunk-17-1.svg\" alt=\"plot of chunk unnamed-chunk-17\">\u003C/p>\n\u003Cp>{% highlight r %}\u003C/p>\n\u003Ch1 id=\"--what-parameter-setting-had-the-best-performance\"># # what parameter setting had the best performance?\u003C/h1>\n\u003Cp>gs_output[[which.max(final_accuracy)]]$params\n{% endhighlight %}\u003C/p>\n\u003Cp>{% highlight text %}\u003C/p>\n\u003Ch2 id=\"beta_val-learning_rate-num_hids\">beta_val learning_rate num_hids\u003C/h2>\n\u003Ch2 id=\"33--------0----------015--------6\">33        0          0.15        6\u003C/h2>\n\u003Cp>{% endhighlight %}\u003C/p>\n\u003Cp>{% highlight r %}\u003C/p>\n\u003Ch1 id=\"--what-parameter-setting-had-the-worst-perfomance\"># # what parameter setting had the worst perfomance?\u003C/h1>\n\u003Cp>gs_output[[which.min(final_accuracy)]]$params\n{% endhighlight %}\u003C/p>\n\u003Cp>{% highlight text %}\u003C/p>\n\u003Ch2 id=\"beta_val-learning_rate-num_hids-1\">beta_val learning_rate num_hids\u003C/h2>\n\u003Ch2 id=\"7--------2-----------01--------4\">7        2           0.1        4\u003C/h2>\n\u003Cp>{% endhighlight %}\u003C/p>\n\u003Cp>{% highlight r %}\u003C/p>\n\u003Ch1 id=\"--plot-em\"># # plot em\u003C/h1>\n\u003Cp>plot_training(list(gs_output[[which.max(final_accuracy)]]$resp_probs,\ngs_output[[which.min(final_accuracy)]]$resp_probs))\n{% endhighlight %}\u003C/p>\n\u003Cp>\u003Cimg src=\"/assets/rfigs/unnamed-chunk-17-2.svg\" alt=\"plot of chunk unnamed-chunk-17\">\u003C/p>\n\u003Cp>{% highlight r %}\u003C/p>\n\u003Ch1 id=\"--what-comes-as-output-for-each-parameter-setting\"># # what comes as output for each parameter setting?\u003C/h1>\n\u003Cp>names(gs_output[[which.max(final_accuracy)]])\n{% endhighlight %}\u003C/p>\n\u003Cp>{% highlight text %}\u003C/p>\n\u003Ch2 id=\"1-resp_probs-params-----st\">[1] “resp_probs” “params”     “st”\u003C/h2>\n\u003Cp>{% endhighlight %}\u003C/p>\n\u003Cp>{% highlight r %}\u003C/p>\n\u003Ch1 id=\"--plot-the-training-curves-for-a-parameter-subset-hid-units--5\"># # plot the training curves for a parameter subset (hid units = 5)\u003C/h1>\n\u003Cp>hidunit5_respprobs &#x3C;- list()\nfor (i in 1:length(gs_output)) {\nif (gs_output[[i]]$params$num_hids == 5){\nhidunit5_respprobs[[paste0(i)]] &#x3C;- gs_output[[i]]$resp_probs\n}\n}\u003C/p>\n\u003Ch1 id=\"--how-many\"># # how many?\u003C/h1>\n\u003Cp>(n_models &#x3C;- length(hidunit5_respprobs))\n{% endhighlight %}\u003C/p>\n\u003Cp>{% highlight text %}\u003C/p>\n\u003Ch2 id=\"1-12\">[1] 12\u003C/h2>\n\u003Cp>{% endhighlight %}\u003C/p>\n\u003Cp>{% highlight r %}\u003C/p>\n\u003Ch1 id=\"--accuracy\"># # accuracy?\u003C/h1>\n\u003Cp>plot(1:n_models, unlist(lapply(hidunit5_respprobs, function(x) x[[12]])))\n{% endhighlight %}\u003C/p>\n\u003Cp>\u003Cimg src=\"/assets/rfigs/unnamed-chunk-17-3.svg\" alt=\"plot of chunk unnamed-chunk-17\">\u003C/p>\n\u003Cp>{% highlight r %}\nplot_training(hidunit5_respprobs)\n{% endhighlight %}\u003C/p>\n\u003Cp>\u003Cimg src=\"/assets/rfigs/unnamed-chunk-17-4.svg\" alt=\"plot of chunk unnamed-chunk-17\">\u003C/p>",{"headings":358,"localImagePaths":450,"remoteImagePaths":451,"frontmatter":452,"imagePaths":453},[359,362,365,368,371,374,377,380,383,386,389,392,395,398,400,403,406,409,412,415,418,421,424,426,429,432,435,438,441,444,447],{"depth":92,"slug":360,"text":361},"-parameter-list","# parameter list",{"depth":92,"slug":363,"text":364},"long_param_list---listbeta_val--c0-1-2-3-4-5-6-7-8-9-10","long_param_list \u003C- list(beta_val = c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10),",{"depth":92,"slug":366,"text":367},"learning_rate--c05-10-15-20-25-35","learning_rate = c(.05, .10, .15, .20, .25, .35),",{"depth":92,"slug":369,"text":370},"num_hids--c4-5-6-10-20","num_hids = c(4, 5, 6, 10, 20))",{"depth":92,"slug":372,"text":373},"--number-of-initializations","# # number of initializations",{"depth":92,"slug":375,"text":376},"--data","# # data",{"depth":92,"slug":378,"text":379},"input_list---get_test_inputstype1","input_list \u003C- get_test_inputs(‘type1’)",{"depth":92,"slug":381,"text":382},"--fit-type","# # fit type",{"depth":92,"slug":384,"text":385},"--fit-vector","# # fit vector",{"depth":92,"slug":387,"text":388},"--single-core","# # single core",{"depth":96,"slug":390,"text":391},"user--system-elapsed","user  system elapsed",{"depth":96,"slug":393,"text":394},"395----000----399","3.95    0.00    3.99",{"depth":92,"slug":396,"text":397},"--parallelized","# # parallelized",{"depth":96,"slug":399,"text":391},"user--system-elapsed-1",{"depth":96,"slug":401,"text":402},"003----000----304","0.03    0.00    3.04",{"depth":92,"slug":404,"text":405},"--how-many-paramter-settings-did-we-have","# # how many paramter settings did we have?",{"depth":96,"slug":407,"text":408},"1-36","[1] 36",{"depth":92,"slug":410,"text":411},"--what-was-the-accuracy-distribution","# # what was the accuracy distribution?",{"depth":92,"slug":413,"text":414},"--what-parameter-setting-had-the-best-performance","# # what parameter setting had the best performance?",{"depth":96,"slug":416,"text":417},"beta_val-learning_rate-num_hids","beta_val learning_rate num_hids",{"depth":96,"slug":419,"text":420},"33--------0----------015--------6","33        0          0.15        6",{"depth":92,"slug":422,"text":423},"--what-parameter-setting-had-the-worst-perfomance","# # what parameter setting had the worst perfomance?",{"depth":96,"slug":425,"text":417},"beta_val-learning_rate-num_hids-1",{"depth":96,"slug":427,"text":428},"7--------2-----------01--------4","7        2           0.1        4",{"depth":92,"slug":430,"text":431},"--plot-em","# # plot em",{"depth":92,"slug":433,"text":434},"--what-comes-as-output-for-each-parameter-setting","# # what comes as output for each parameter setting?",{"depth":96,"slug":436,"text":437},"1-resp_probs-params-----st","[1] “resp_probs” “params”     “st”",{"depth":92,"slug":439,"text":440},"--plot-the-training-curves-for-a-parameter-subset-hid-units--5","# # plot the training curves for a parameter subset (hid units = 5)",{"depth":92,"slug":442,"text":443},"--how-many","# # how many?",{"depth":96,"slug":445,"text":446},"1-12","[1] 12",{"depth":92,"slug":448,"text":449},"--accuracy","# # accuracy?",[],[],{"title":349,"excerpt":350,"tags":351,"season":206,"type":9},[],"2017-7-30-catlearn-grid-search.md","2012-8-29-safest-route-to-bing",{"id":455,"data":457,"body":462,"filePath":463,"digest":464,"rendered":465,"legacyId":473},{"title":458,"excerpt":459,"tags":460,"season":461,"type":9},"Safest Route to Binghamton University","Safest Bike Route from Binghamton (city) to Binghamton (university)","cycling, binghamton, biking","summer 2012","Hello! Hopefully you found this page because you were looking for a convenient and safe route to Binghamton University from anywhere in Binghamton \"proper\". \n\n*Some background.* This is the safest route I have found to get to Binghamton University assuming you're coming from the general vicinity of central Binghamton. After several years' experience biking in much larger cities, I was surprised to find that biking in Binghamton made me uncomfortable at times, and this route helped to alleviate much of that stress (at least for my daily commute). \n\nThere are a few modest climbs, but after a few trips you'll hardly even notice them! If you *really* want to get your blood pumping there are a few places you can take a left off of Moore to add a bit more climbing to the trip.\n\n*Note.* This route is most efficient if you are east of, say, Beethoven Street on Binghamton's west side. Even if you aren't you might prefer this route---especially in the winter when the bike lanes and sidewalks of the 201 bridge are not cleared of ice and debris (yea, like all winter). Happy Trails!\n\n*Update.* A fully-featured dedicated bike path [is in the works](http://www.pressconnects.com/story/news/local/2016/04/29/trail-blazing-16m-pedestrian-route-would-link-bu-downtown/82344124/) to link Binghamton University to downtown, so thank your government officials for addressing this problem. \n\n\u003Ciframe src=\"https://www.google.com/maps/d/u/0/embed?mid=1OkUodDDf2kCKFbrXyYXtxfb0aqUo_Cdr\" width=\"640\" height=\"480\">\u003C/iframe>\n\n\n\nStep-by-step directions:\n\n\u003Ctable style=\"width:100%\">\n  \u003Ctr>\n    \u003Cth>Direction\u003C/th>\n    \u003Cth>Distance\u003C/th> \n  \u003C/tr>\n  \u003Ctr>\n    \u003Ctd>Take the pedestrian bridge across the Susquehanna River\u003C/td>\n    \u003Ctd>\u003C/td> \n  \u003C/tr>\n  \n    \u003Ctr>\n    \u003Ctd>Turn right onto Pennsylvania Ave\u003C/td>\n    \u003Ctd>92 ft\u003C/td> \n  \u003C/tr>\n  \n    \u003Ctr>\n    \u003Ctd>Slight left to stay on Pennsylvania Ave\u003C/td>\n    \u003Ctd>0.694 mi\u003C/td> \n  \u003C/tr>\n  \n    \u003Ctr>\n    \u003Ctd>Turn right onto Moore Ave\u003C/td>\n    \u003Ctd>0.873 mi\u003C/td> \n  \u003C/tr>\n  \n    \u003Ctr>\n    \u003Ctd>Turn right onto Hazard Hill Rd\u003C/td>\n    \u003Ctd>262 ft\u003C/td> \n  \u003C/tr>\n  \n    \u003Ctr>\n    \u003Ctd>Continue onto Hawthorne Rd\u003C/td>\n    \u003Ctd>1,168 ft\u003C/td> \n  \u003C/tr>\n  \n    \u003Ctr>\n    \u003Ctd>Turn left onto Clifton Blvd\u003C/td>\n    \u003Ctd>0.925 mi\u003C/td> \n  \u003C/tr>\n  \n    \u003Ctr>\n    \u003Ctd>Turn left onto Clubhouse Rd\u003C/td>\n    \u003Ctd>0.413 mi\u003C/td> \n  \u003C/tr>\n  \n    \u003Ctr>\n    \u003Ctd>Turn left onto Country Club Rd\u003C/td>\n    \u003Ctd>0.314 mi\u003C/td> \n  \u003C/tr>\n  \n    \u003Ctr>\n    \u003Ctd>Turn right onto Washington Dr\u003C/td>\n    \u003Ctd>0.290 mi\u003C/td> \n  \u003C/tr>\n  \n    \u003Ctr>\n    \u003Ctd>Turn right onto Lehigh Ave\u003C/td>\n    \u003Ctd>341 ft\u003C/td> \n  \u003C/tr>\n  \n    \u003Ctr>\n    \u003Ctd>Continue onto E Access Rd til you're there!\u003C/td>\n    \u003Ctd>\u003C/td> \n  \u003C/tr>\n\n\u003C/table>","src/content/blog/2012-8-29-safest-route-to-bing.md","bb10b10b5343c471",{"html":466,"metadata":467},"\u003Cp>Hello! Hopefully you found this page because you were looking for a convenient and safe route to Binghamton University from anywhere in Binghamton “proper”.\u003C/p>\n\u003Cp>\u003Cem>Some background.\u003C/em> This is the safest route I have found to get to Binghamton University assuming you’re coming from the general vicinity of central Binghamton. After several years’ experience biking in much larger cities, I was surprised to find that biking in Binghamton made me uncomfortable at times, and this route helped to alleviate much of that stress (at least for my daily commute).\u003C/p>\n\u003Cp>There are a few modest climbs, but after a few trips you’ll hardly even notice them! If you \u003Cem>really\u003C/em> want to get your blood pumping there are a few places you can take a left off of Moore to add a bit more climbing to the trip.\u003C/p>\n\u003Cp>\u003Cem>Note.\u003C/em> This route is most efficient if you are east of, say, Beethoven Street on Binghamton’s west side. Even if you aren’t you might prefer this route---especially in the winter when the bike lanes and sidewalks of the 201 bridge are not cleared of ice and debris (yea, like all winter). Happy Trails!\u003C/p>\n\u003Cp>\u003Cem>Update.\u003C/em> A fully-featured dedicated bike path \u003Ca href=\"http://www.pressconnects.com/story/news/local/2016/04/29/trail-blazing-16m-pedestrian-route-would-link-bu-downtown/82344124/\">is in the works\u003C/a> to link Binghamton University to downtown, so thank your government officials for addressing this problem.\u003C/p>\n\u003Ciframe src=\"https://www.google.com/maps/d/u/0/embed?mid=1OkUodDDf2kCKFbrXyYXtxfb0aqUo_Cdr\" width=\"640\" height=\"480\">\u003C/iframe>\n\u003Cp>Step-by-step directions:\u003C/p>\n\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>&#x3C;tr>\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>&#x3C;td>Turn right onto Pennsylvania Ave&#x3C;/td>\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>&#x3C;td>92 ft&#x3C;/td> \u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n  \n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>&#x3C;tr>\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>&#x3C;td>Slight left to stay on Pennsylvania Ave&#x3C;/td>\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>&#x3C;td>0.694 mi&#x3C;/td> \u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n  \n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>&#x3C;tr>\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>&#x3C;td>Turn right onto Moore Ave&#x3C;/td>\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>&#x3C;td>0.873 mi&#x3C;/td> \u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n  \n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>&#x3C;tr>\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>&#x3C;td>Turn right onto Hazard Hill Rd&#x3C;/td>\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>&#x3C;td>262 ft&#x3C;/td> \u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n  \n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>&#x3C;tr>\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>&#x3C;td>Continue onto Hawthorne Rd&#x3C;/td>\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>&#x3C;td>1,168 ft&#x3C;/td> \u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n  \n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>&#x3C;tr>\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>&#x3C;td>Turn left onto Clifton Blvd&#x3C;/td>\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>&#x3C;td>0.925 mi&#x3C;/td> \u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n  \n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>&#x3C;tr>\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>&#x3C;td>Turn left onto Clubhouse Rd&#x3C;/td>\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>&#x3C;td>0.413 mi&#x3C;/td> \u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n  \n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>&#x3C;tr>\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>&#x3C;td>Turn left onto Country Club Rd&#x3C;/td>\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>&#x3C;td>0.314 mi&#x3C;/td> \u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n  \n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>&#x3C;tr>\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>&#x3C;td>Turn right onto Washington Dr&#x3C;/td>\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>&#x3C;td>0.290 mi&#x3C;/td> \u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n  \n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>&#x3C;tr>\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>&#x3C;td>Turn right onto Lehigh Ave&#x3C;/td>\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>&#x3C;td>341 ft&#x3C;/td> \u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n  \n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>&#x3C;tr>\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>&#x3C;td>Continue onto E Access Rd til you're there!&#x3C;/td>\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>&#x3C;td>&#x3C;/td> \u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n  \n\u003Ctable style=\"width:100%\">\n  \u003Ctbody>\u003Ctr>\n    \u003Cth>Direction\u003C/th>\n    \u003Cth>Distance\u003C/th> \n  \u003C/tr>\n  \u003Ctr>\n    \u003Ctd>Take the pedestrian bridge across the Susquehanna River\u003C/td>\n    \u003Ctd>\u003C/td> \n  \u003C/tr>\u003C/tbody>\u003C/table>",{"headings":468,"localImagePaths":469,"remoteImagePaths":470,"frontmatter":471,"imagePaths":472},[],[],[],{"title":458,"excerpt":459,"tags":460,"season":461,"type":9},[],"2012-8-29-safest-route-to-bing.md","2017-9-1-catcon-preprint",{"id":474,"data":476,"body":481,"filePath":482,"digest":483,"rendered":484,"legacyId":492},{"title":477,"excerpt":478,"tags":479,"season":153,"assets":18,"type":19,"external_url":480},"Preprint - Category Construction Promotes Transfer","Submitted the first preprint from the LaRC Lab - Category Construction Promotes Transfer","transfer, cognition, paper","/pdfs/manuscripts/kurtz-and-honke-psyarxiv-catcon.pdf","Yay open science","src/content/blog/2017-9-1-catcon-preprint.md","8b7f16b7dd0c4b74",{"html":485,"metadata":486},"\u003Cp>Yay open science\u003C/p>",{"headings":487,"localImagePaths":488,"remoteImagePaths":489,"frontmatter":490,"imagePaths":491},[],[],[],{"title":477,"excerpt":478,"tags":479,"season":153,"assets":18,"type":19,"external_url":480},[],"2017-9-1-catcon-preprint.md","2018-3-23-croc",{"id":493,"data":495,"filePath":501,"digest":502,"rendered":503,"legacyId":510},{"title":496,"excerpt":497,"tags":498,"season":499,"assets":18,"type":172,"external_url":500},"CROC (Character Recognition and Object Classification) Image Analysis System","Published an open-source PIP-installable library that performs object recognition and OCR","python, neural network, machine learning","spring 2018","https://github.com/NewKnowledge/nk_croc","src/content/blog/2018-3-23-croc.md","78fee088be1bad3a",{"html":24,"metadata":504},{"headings":505,"localImagePaths":506,"remoteImagePaths":507,"frontmatter":508,"imagePaths":509},[],[],[],{"title":496,"excerpt":497,"tags":498,"season":499,"assets":18,"type":172,"external_url":500},[],"2018-3-23-croc.md","2018-4-13-relational-categories-symposium18",{"id":511,"data":513,"filePath":518,"digest":519,"rendered":520,"legacyId":527},{"title":514,"excerpt":515,"tags":516,"season":499,"assets":18,"type":19,"external_url":517},"Relational Categories: Why they're Important and How they are Learned","Symposium on relational category learning accepted at CogSci 2018","relational knowledge, cognition, paper, category","/pdfs/manuscripts/gentner_2_1.pdf","src/content/blog/2018-4-13-relational-categories-symposium18.md","8641a741d33c9755",{"html":24,"metadata":521},{"headings":522,"localImagePaths":523,"remoteImagePaths":524,"frontmatter":525,"imagePaths":526},[],[],[],{"title":514,"excerpt":515,"tags":516,"season":499,"assets":18,"type":19,"external_url":517},[],"2018-4-13-relational-categories-symposium18.md","2018-4-23-ape",{"id":528,"data":530,"filePath":535,"digest":536,"rendered":537,"legacyId":544},{"title":531,"excerpt":532,"tags":533,"season":499,"assets":18,"type":172,"external_url":534},"APE - Abstractive Prediction via [knowledge] Embedding","Colleagues and I published APE (Abstractive Prediction via Embedding), an open-source PIP-installable library that predicts more abstract labels (i.e. higher-order category membership) for a provided set of concepts","python, categories, machine learning","https://github.com/NewKnowledge/nk_ape","src/content/blog/2018-4-23-ape.md","6b9dd73245edac2b",{"html":24,"metadata":538},{"headings":539,"localImagePaths":540,"remoteImagePaths":541,"frontmatter":542,"imagePaths":543},[],[],[],{"title":531,"excerpt":532,"tags":533,"season":499,"assets":18,"type":172,"external_url":534},[],"2018-4-23-ape.md","2018-6-16-duke_automl18",{"id":545,"data":547,"filePath":553,"digest":554,"rendered":555,"legacyId":562},{"title":548,"excerpt":549,"tags":550,"season":551,"assets":18,"type":19,"external_url":552},"Abstractive Tabular Dataset Summarization via Knowledge Base Semantic Embeddings","Paper on automatic dataset topic summarization accepted for presentation at the ICML '18 AutoML workshop","machine learning, paper","summer 2018","/pdfs/manuscripts/azunre_et_al_2018.pdf","src/content/blog/2018-6-16-duke_automl18.md","68025f3d99f9c340",{"html":24,"metadata":556},{"headings":557,"localImagePaths":558,"remoteImagePaths":559,"frontmatter":560,"imagePaths":561},[],[],[],{"title":548,"excerpt":549,"tags":550,"season":551,"assets":18,"type":19,"external_url":552},[],"2018-6-16-duke_automl18.md","2018-6-24-ibex",{"id":563,"data":565,"filePath":570,"digest":571,"rendered":572,"legacyId":579},{"title":566,"excerpt":567,"tags":568,"season":551,"assets":18,"type":172,"external_url":569},"IBEX - Intelligence-Based Entity Xtraction","Colleagues and I (mostly them!) have open-sourced IBEX, a topic extraction library in python","python, topic extraction, machine learning","https://github.com/NewKnowledge/ibex","src/content/blog/2018-6-24-ibex.md","c267c785faf2d3ea",{"html":24,"metadata":573},{"headings":574,"localImagePaths":575,"remoteImagePaths":576,"frontmatter":577,"imagePaths":578},[],[],[],{"title":566,"excerpt":567,"tags":568,"season":551,"assets":18,"type":172,"external_url":569},[],"2018-6-24-ibex.md","2018-6-26-honke_and_kurtz_18",{"id":580,"data":582,"filePath":587,"digest":588,"rendered":589,"legacyId":596},{"title":583,"excerpt":584,"tags":585,"season":551,"assets":18,"type":19,"external_url":586},"Similarity is as Similarity Does? A Critical Inquiry into the Effect of Thematic Association on Similarity","The first behavioral work from my dissertation is under review (preprint at PsyArXiv) wherein we discovered a surprising inconsistency in human similarity judgements across the timecourse of a single experimental session","similarity, cognition, paper","/pdfs/manuscripts/honke-and-kurtz-2018.pdf","src/content/blog/2018-6-26-honke_and_kurtz_18.md","114bbc9f0f9ea5d9",{"html":24,"metadata":590},{"headings":591,"localImagePaths":592,"remoteImagePaths":593,"frontmatter":594,"imagePaths":595},[],[],[],{"title":583,"excerpt":584,"tags":585,"season":551,"assets":18,"type":19,"external_url":586},[],"2018-6-26-honke_and_kurtz_18.md","2018-6-23-unicorn",{"id":597,"data":599,"filePath":604,"digest":605,"rendered":606,"legacyId":613},{"title":600,"excerpt":601,"tags":602,"season":551,"assets":18,"type":172,"external_url":603},"UNICORN - UNsupervised Image Clustering with Object Recognition Network","Published UNICORN (UNsupervised Image Clustering with Object Recognition Network), a python library that classifies images based on their similarity with respect to the activation pattern produced by the Inception V3 network trained on Imagenet.","python, image analysis, machine learning","https://github.com/NewKnowledge/nk_unicorn","src/content/blog/2018-6-23-unicorn.md","2cf9c81e653814fd",{"html":24,"metadata":607},{"headings":608,"localImagePaths":609,"remoteImagePaths":610,"frontmatter":611,"imagePaths":612},[],[],[],{"title":600,"excerpt":601,"tags":602,"season":551,"assets":18,"type":172,"external_url":603},[],"2018-6-23-unicorn.md","2018-9-04-category_conflict",{"id":614,"data":616,"filePath":622,"digest":623,"rendered":624,"legacyId":631},{"title":617,"excerpt":618,"tags":619,"season":620,"assets":18,"type":19,"external_url":621},"Categories in Conflict: Combating the application of an intuitive conception of inheritance with category construction.","Paper published at the Journal of Research in Science Teaching - category construction in the classroom as a technique to reduce the use of scientifically-invalid (i.e., intuitive) beliefs about inheritance","transfer, cognition, learning, paper","fall 2018","/pdfs/manuscripts/premo-et-al-2018.pdf","src/content/blog/2018-9-04-category_conflict.md","d13598d2e278db1f",{"html":24,"metadata":625},{"headings":626,"localImagePaths":627,"remoteImagePaths":628,"frontmatter":629,"imagePaths":630},[],[],[],{"title":617,"excerpt":618,"tags":619,"season":620,"assets":18,"type":19,"external_url":621},[],"2018-9-04-category_conflict.md","2019-04-20-ocr-on-gov-docs",{"id":632,"data":634,"filePath":640,"digest":641,"rendered":642,"legacyId":649},{"title":635,"excerpt":636,"tags":637,"season":638,"assets":18,"type":9,"external_url":639},"A short thread on efforts to re-digitize the un-digitized","Short thread with resources on digitization, government records, and publishing decisions of the Mueller Report by the U.S. DOJ","incarceration trends, campaign finance, OCR, code","spring 2019","https://twitter.com/garretthonke/status/1119623619146125314","src/content/blog/2019-04-20-ocr-on-gov-docs.md","3457e4a33832eff7",{"html":24,"metadata":643},{"headings":644,"localImagePaths":645,"remoteImagePaths":646,"frontmatter":647,"imagePaths":648},[],[],[],{"title":635,"excerpt":636,"tags":637,"season":638,"assets":18,"type":9,"external_url":639},[],"2019-04-20-ocr-on-gov-docs.md","2019-06-15-blog-slowdown",{"id":650,"data":652,"body":656,"filePath":657,"digest":658,"rendered":659,"legacyId":667},{"title":653,"excerpt":654,"season":655,"assets":18,"type":9},"Solve for X","Accepted a research scientist role focused on computational neursoscience and ml with a project in stealth mode housed at X (Alphabet's moonshot factory).","summer 2019","And that's all I can say about that.","src/content/blog/2019-06-15-blog-slowdown.md","5850ce716614c30a",{"html":660,"metadata":661},"\u003Cp>And that’s all I can say about that.\u003C/p>",{"headings":662,"localImagePaths":663,"remoteImagePaths":664,"frontmatter":665,"imagePaths":666},[],[],[],{"title":653,"excerpt":654,"season":655,"assets":18,"type":9},[],"2019-06-15-blog-slowdown.md","2019-1-20-similarity_is_as_similarity_does",{"id":668,"data":670,"filePath":676,"digest":677,"rendered":678,"legacyId":685},{"title":671,"excerpt":672,"tags":673,"season":674,"assets":18,"type":19,"external_url":675},"Similarity is as similarity does? A critical inquiry into the effect of thematic association on similarity","Paper published at Cognition: Similarity is as similarity does? A critical inquiry into the effect of thematic association on similarity. This work provides a better understanding of the curious case of thematic similarity, where thematic integration-driven similarity judgment behavior is found to be less frequent than thought.","cognition, similarity, thematic, taxonomic, categories, paper","winter 2019","http://kurtzlab.psychology.binghamton.edu/publications/honke%20and%20kurtz%202019.pdf","src/content/blog/2019-1-20-similarity_is_as_similarity_does.md","e8ada7b3b4ba5ba8",{"html":24,"metadata":679},{"headings":680,"localImagePaths":681,"remoteImagePaths":682,"frontmatter":683,"imagePaths":684},[],[],[],{"title":671,"excerpt":672,"tags":673,"season":674,"assets":18,"type":19,"external_url":675},[],"2019-1-20-similarity_is_as_similarity_does.md","2019-10-01-disinfo-framework",{"id":686,"data":688,"filePath":694,"digest":695,"rendered":696,"legacyId":703},{"title":689,"excerpt":690,"tags":691,"season":692,"assets":18,"type":19,"external_url":693},"Disinformation: Detect to Disrupt","Our framework for combating online disinformation accepted to the Comparative Approaches to Disinformation workshop hosted by the Berkman Klien Center for Internet and Society.","disinformation","fall 2019","https://cyber.harvard.edu/story/2019-10/comparative-approaches-disinformation","src/content/blog/2019-10-01-disinfo-framework.md","a012923da30a90df",{"html":24,"metadata":697},{"headings":698,"localImagePaths":699,"remoteImagePaths":700,"frontmatter":701,"imagePaths":702},[],[],[],{"title":689,"excerpt":690,"tags":691,"season":692,"assets":18,"type":19,"external_url":693},[],"2019-10-01-disinfo-framework.md","2019-7-20-kurtz_and_honke_2019",{"id":704,"data":706,"filePath":711,"digest":712,"rendered":713,"legacyId":720},{"title":707,"excerpt":708,"tags":709,"season":655,"assets":18,"type":19,"external_url":710},"Sorting Out the Problem of Inert Knowledge: Category Construction to Promote Spontaneous Transfer.","Our paper introducing the category status hypothesis for knowledge transfer accepted at JEP:LMC. We show in a series of experiments that learning a novel relational category through category construction improves spontaneous transfer.","similarity, cognition, categories, transfer, paper","/pdfs/manuscripts/kurtz-and-honke-2019-category-status.pdf","src/content/blog/2019-7-20-kurtz_and_honke_2019.md","bf7b87658f176c19",{"html":24,"metadata":714},{"headings":715,"localImagePaths":716,"remoteImagePaths":717,"frontmatter":718,"imagePaths":719},[],[],[],{"title":707,"excerpt":708,"tags":709,"season":655,"assets":18,"type":19,"external_url":710},[],"2019-7-20-kurtz_and_honke_2019.md","2019-7-22-honke_kurtz_and_laszlo_2019",{"id":721,"data":723,"filePath":728,"digest":729,"rendered":730,"legacyId":737},{"title":724,"excerpt":725,"tags":726,"season":655,"assets":18,"type":19,"external_url":727},"Similarity Judgments Predict N400 Amplitude Differences between Taxonomic Category Members and Thematic Associates.","Submitted our latest on semantic knowledge organization (preprint at PsyArXiv) that provides evidence that N400 amplitude reliably differs for processing of taxonomic catgeory members and thematic associates; critically, the amplitude delta varies consistently with people's offline similarity judgment behavior.","similarity, cognition, neuroscience, paper","/pdfs/manuscripts/honke-et-al-psyarxiv-n400-similarity.pdf","src/content/blog/2019-7-22-honke_kurtz_and_laszlo_2019.md","d6d60149e7d76b93",{"html":24,"metadata":731},{"headings":732,"localImagePaths":733,"remoteImagePaths":734,"frontmatter":735,"imagePaths":736},[],[],[],{"title":724,"excerpt":725,"tags":726,"season":655,"assets":18,"type":19,"external_url":727},[],"2019-7-22-honke_kurtz_and_laszlo_2019.md","2020-02-01-taxonomic-similarity-and-n400-amplitude",{"id":738,"data":740,"filePath":745,"digest":746,"rendered":747,"legacyId":754},{"title":741,"excerpt":742,"tags":726,"season":743,"assets":18,"type":19,"external_url":744},"Similarity Judgments Predict N400 Amplitude Differences Accepted for Publication at Neuropsychologia","Paper on the correspondence between N400 amplitude and similarity processing accepted for publication at Neuropsychologia.","winter 2020","/pdfs/manuscripts/honke-etal-2020.pdf","src/content/blog/2020-02-01-taxonomic-similarity-and-n400-amplitude.md","eea31f5b0cac08bb",{"html":24,"metadata":748},{"headings":749,"localImagePaths":750,"remoteImagePaths":751,"frontmatter":752,"imagePaths":753},[],[],[],{"title":741,"excerpt":742,"tags":726,"season":743,"assets":18,"type":19,"external_url":744},[],"2020-02-01-taxonomic-similarity-and-n400-amplitude.md","2020-12-10-bvae-actigraphy-neurips-workshop",{"id":755,"data":757,"filePath":762,"digest":763,"rendered":764,"legacyId":771},{"title":758,"excerpt":759,"season":743,"assets":18,"tags":760,"type":19,"external_url":761},"Using Convolutional Variational Autoencoders to Predict Post-Trauma Health Outcomes from Actigraphy Data","Spotlight talk accepted at the NeuRIPS 2021 Machine Learning for Mobile Health workshop: Using Convolutional Variational Autoencoders to Predict Post-Trauma Health Outcomes from Actigraphy Data.","machine learning, unsupervised learning, psychopathology, actigraphy","https://nips.cc/virtual/2020/19556","src/content/blog/2020-12-10-bvae-actigraphy-neurips-workshop.md","e0c8bfdc7d8083ef",{"html":24,"metadata":765},{"headings":766,"localImagePaths":767,"remoteImagePaths":768,"frontmatter":769,"imagePaths":770},[],[],[],{"title":758,"excerpt":759,"season":743,"assets":18,"tags":760,"type":19,"external_url":761},[],"2020-12-10-bvae-actigraphy-neurips-workshop.md","2019-5-25-dhamani-et-al-aisg-workshop",{"id":772,"data":774,"filePath":779,"digest":780,"rendered":781,"legacyId":788},{"title":775,"excerpt":776,"tags":777,"season":638,"assets":18,"type":19,"external_url":778},"Using Deep Networks and Transfer Learning to Address Disinformation","Paper accepted for presentation at ICML '19 AI for Social Good Workshop. The project tested an ensemble CNN-LSTM pipeline for text classification.","machine learning, CNN, LSTM, disinformation, paper","https://arxiv.org/pdf/1905.10412.pdf","src/content/blog/2019-5-25-dhamani-et-al-aisg-workshop.md","95dfe1350fdbd781",{"html":24,"metadata":782},{"headings":783,"localImagePaths":784,"remoteImagePaths":785,"frontmatter":786,"imagePaths":787},[],[],[],{"title":775,"excerpt":776,"tags":777,"season":638,"assets":18,"type":19,"external_url":778},[],"2019-5-25-dhamani-et-al-aisg-workshop.md","2019-1-24-simon",{"id":789,"data":791,"filePath":796,"digest":797,"rendered":798,"legacyId":805},{"title":792,"excerpt":793,"tags":794,"season":674,"assets":18,"type":19,"external_url":795},"SIMON - Semantic Inference for the Modeling of ONtologies","Colleagues and I submitted SIMON (preprint on arXiv). SIMON is a semantic classifier that uses the features of a character-level convolutional neural network to predict class membership. The algorithm shows impressive performance on tasks as diverse as social media account user age, tabular data column types (e.g., GPS coordinates, ordinal values, postal codes, etc.), and Spam classification.","python, categories, classification, machine learning, deep learning","https://arxiv.org/pdf/1901.08456.pdf","src/content/blog/2019-1-24-simon.md","8deabd2880ec4512",{"html":24,"metadata":799},{"headings":800,"localImagePaths":801,"remoteImagePaths":802,"frontmatter":803,"imagePaths":804},[],[],[],{"title":792,"excerpt":793,"tags":794,"season":674,"assets":18,"type":19,"external_url":795},[],"2019-1-24-simon.md","2020-11-02-amber-opensources",{"id":806,"data":808,"body":813,"filePath":814,"digest":815,"rendered":816,"legacyId":824},{"title":809,"excerpt":810,"season":811,"assets":18,"type":9,"external_url":812},"Amber shutdown","My Main project at X was opensourced today: Sharing Project Amber with the mental health community.","fall 2020","https://blog.x.company/sharing-project-amber-with-the-mental-health-community-7b6d8814a862?gi=d269516144d0","My Main project at X was opensourced today: Sharing Project Amber with the mental health community","src/content/blog/2020-11-02-amber-opensources.md","38b9445330f225f1",{"html":817,"metadata":818},"\u003Cp>My Main project at X was opensourced today: Sharing Project Amber with the mental health community\u003C/p>",{"headings":819,"localImagePaths":820,"remoteImagePaths":821,"frontmatter":822,"imagePaths":823},[],[],[],{"title":809,"excerpt":810,"season":811,"assets":18,"type":9,"external_url":812},[],"2020-11-02-amber-opensources.md","2021-05-01-bvae-for-eeg",{"id":825,"data":827,"filePath":833,"digest":834,"rendered":835,"legacyId":842},{"title":828,"excerpt":829,"season":830,"assets":18,"tags":831,"type":19,"external_url":832},"REPRESENTATION LEARNING FOR IMPROVED INTERPRETABILITY AND CLASSIFICATION ACCURACY OF CLINICAL FACTORS FROM EEG","Our paper on bVAEs and disentanglement for intepretability and classification of clinical factors from EEG presented at ICLR 2021.","spring 2021","machine learning, unsupervised learning, psychopathology","https://openreview.net/pdf?id=TVjLza1t4hI","src/content/blog/2021-05-01-bvae-for-eeg.md","7c224192905246f7",{"html":24,"metadata":836},{"headings":837,"localImagePaths":838,"remoteImagePaths":839,"frontmatter":840,"imagePaths":841},[],[],[],{"title":828,"excerpt":829,"season":830,"assets":18,"tags":831,"type":19,"external_url":832},[],"2021-05-01-bvae-for-eeg.md","2021-2-01-joined-cortex",{"id":843,"data":845,"body":847,"filePath":849,"digest":850,"rendered":851,"legacyId":859},{"title":846,"excerpt":847,"season":848,"assets":18,"type":9},"Joined the Cortex team","I joined the Cortex team at X. We do math, science, and machine learning for zero-to-one projects.","winter 2021","src/content/blog/2021-2-01-joined-cortex.md","c6cb26c784dcf7aa",{"html":852,"metadata":853},"\u003Cp>I joined the Cortex team at X. We do math, science, and machine learning for zero-to-one projects.\u003C/p>",{"headings":854,"localImagePaths":855,"remoteImagePaths":856,"frontmatter":857,"imagePaths":858},[],[],[],{"title":846,"excerpt":847,"season":848,"assets":18,"type":9},[],"2021-2-01-joined-cortex.md","2021-8-1-reservoir_nn_oss",{"id":860,"data":862,"body":866,"filePath":867,"digest":868,"rendered":869,"legacyId":877},{"title":863,"excerpt":864,"season":865,"assets":18,"type":9},"Valkyrie Reservoir_NN Package Released","We open-source our work enabling the use of adaptable reservoir layers for building neural networks in Keras. BYOR - Bring your own reservoir!","summer 2021","Check out the `reservoir_nn` package that enables user to BYOR - Bring your won reservoir! - and use the connectivity matrix as a plug-and-play layer in any arbitrary neural network built in Keras. Find the library [here](https://github.com/keras-team/reservoir_nn).","src/content/blog/2021-8-1-reservoir_nn_oss.md","8c74a3977c20b0b1",{"html":870,"metadata":871},"\u003Cp>Check out the \u003Ccode>reservoir_nn\u003C/code> package that enables user to BYOR - Bring your won reservoir! - and use the connectivity matrix as a plug-and-play layer in any arbitrary neural network built in Keras. Find the library \u003Ca href=\"https://github.com/keras-team/reservoir_nn\">here\u003C/a>.\u003C/p>",{"headings":872,"localImagePaths":873,"remoteImagePaths":874,"frontmatter":875,"imagePaths":876},[],[],[],{"title":863,"excerpt":864,"season":865,"assets":18,"type":9},[],"2021-8-1-reservoir_nn_oss.md","2022-06-02-reservoir_project-patents",{"id":878,"data":880,"body":884,"filePath":885,"digest":886,"rendered":887,"legacyId":895},{"title":881,"excerpt":882,"season":883,"assets":18,"type":9},"Reservoir  Patents","Patents filed for our effort to deploy facsimiles of real biological connectomes as interchangeable reservoirs in artificial neural networks.","summer 2022","This project investigated the effectiveness of using the connectivity structure of real biological connectomes (e.g., Drosophila) as interchangeable reservoir layers in SoTA neural architectures like UNets, LSTMs, and Transformers.  \n\n*Ensemble machine learning with reservoir neural networks* \n\n[Here](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=WPewiKcAAAAJ&sortby=pubdate&citation_for_view=WPewiKcAAAAJ:vRqMK49ujn8C) \n\n*Attention-based brain emulation neural netowrks* \n\n[Here](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=WPewiKcAAAAJ&sortby=pubdate&citation_for_view=WPewiKcAAAAJ:l7t_Zn2s7bgC)","src/content/blog/2022-06-02-reservoir_project-patents.md","444df99fc40ecb90",{"html":888,"metadata":889},"\u003Cp>This project investigated the effectiveness of using the connectivity structure of real biological connectomes (e.g., Drosophila) as interchangeable reservoir layers in SoTA neural architectures like UNets, LSTMs, and Transformers.\u003C/p>\n\u003Cp>\u003Cem>Ensemble machine learning with reservoir neural networks\u003C/em>\u003C/p>\n\u003Cp>\u003Ca href=\"https://scholar.google.com/citations?view_op=view_citation&#x26;hl=en&#x26;user=WPewiKcAAAAJ&#x26;sortby=pubdate&#x26;citation_for_view=WPewiKcAAAAJ:vRqMK49ujn8C\">Here\u003C/a>\u003C/p>\n\u003Cp>\u003Cem>Attention-based brain emulation neural netowrks\u003C/em>\u003C/p>\n\u003Cp>\u003Ca href=\"https://scholar.google.com/citations?view_op=view_citation&#x26;hl=en&#x26;user=WPewiKcAAAAJ&#x26;sortby=pubdate&#x26;citation_for_view=WPewiKcAAAAJ:l7t_Zn2s7bgC\">Here\u003C/a>\u003C/p>",{"headings":890,"localImagePaths":891,"remoteImagePaths":892,"frontmatter":893,"imagePaths":894},[],[],[],{"title":881,"excerpt":882,"season":883,"assets":18,"type":9},[],"2022-06-02-reservoir_project-patents.md","2022-03-01-amber-patents",{"id":896,"data":898,"body":902,"filePath":903,"digest":904,"rendered":905,"legacyId":913},{"title":899,"excerpt":900,"season":901,"assets":18,"type":9},"Amber Patents","We filed a ton of patents for technology supporting our work on advanced diagnostics and monitoring for mental health including filings on the earliest known use of transformer architectures for electrophysiological representation learning.","spring 2022","We filed a ton of patents for technology supporting our work on advanced diagnostics and monitoring for mental health including filings on the earliest known use of transformer architectures for electrophsiological representation learning.\n\n*Latent Factor Structuring of Psychopathology* \n\n[Here](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=WPewiKcAAAAJ&sortby=pubdate&citation_for_view=WPewiKcAAAAJ:UxriW0iASnsC) \n\n*De-noising task-specific electroencephalogram signals using neural networks* \n\n[Here](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=WPewiKcAAAAJ&sortby=pubdate&citation_for_view=WPewiKcAAAAJ:SP6oXDckpogC)\n\n*Processing time-frequency representations of EEG data using neural networks* \n\n[Here](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=WPewiKcAAAAJ&sortby=pubdate&citation_for_view=WPewiKcAAAAJ:uWQEDVKXjbEC)\n\n*Processing EEG data with twin neural networks* \n\n[Here](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=WPewiKcAAAAJ&sortby=pubdate&citation_for_view=WPewiKcAAAAJ:OU6Ihb5iCvQC)\n\n*Attention encoding stack in EEG trial aggregation* \n\n[Here](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=WPewiKcAAAAJ&sortby=pubdate&citation_for_view=WPewiKcAAAAJ:p2g8aNsByqUC)\n\n*Attention encoding stack in aggregation of data* \n\n[Here](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=WPewiKcAAAAJ&sortby=pubdate&citation_for_view=WPewiKcAAAAJ:XiSMed-E-HIC)\n\n*EEG signal representations using auto-encoders* \n\n[Here](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=WPewiKcAAAAJ&sortby=pubdate&citation_for_view=WPewiKcAAAAJ:u9iWguZQMMsC)\n\n*Resampling EEG trial data* \n\n[Here](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=WPewiKcAAAAJ&sortby=pubdate&citation_for_view=WPewiKcAAAAJ:Tiz5es2fbqcC)\n\n*Processing time-domain and frequency-domain representations of EEG data* \n\n[Here](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=WPewiKcAAAAJ&sortby=pubdate&citation_for_view=WPewiKcAAAAJ:WbkHhVStYXYCC)","src/content/blog/2022-03-01-amber-patents.md","4971991ffe31defd",{"html":906,"metadata":907},"\u003Cp>We filed a ton of patents for technology supporting our work on advanced diagnostics and monitoring for mental health including filings on the earliest known use of transformer architectures for electrophsiological representation learning.\u003C/p>\n\u003Cp>\u003Cem>Latent Factor Structuring of Psychopathology\u003C/em>\u003C/p>\n\u003Cp>\u003Ca href=\"https://scholar.google.com/citations?view_op=view_citation&#x26;hl=en&#x26;user=WPewiKcAAAAJ&#x26;sortby=pubdate&#x26;citation_for_view=WPewiKcAAAAJ:UxriW0iASnsC\">Here\u003C/a>\u003C/p>\n\u003Cp>\u003Cem>De-noising task-specific electroencephalogram signals using neural networks\u003C/em>\u003C/p>\n\u003Cp>\u003Ca href=\"https://scholar.google.com/citations?view_op=view_citation&#x26;hl=en&#x26;user=WPewiKcAAAAJ&#x26;sortby=pubdate&#x26;citation_for_view=WPewiKcAAAAJ:SP6oXDckpogC\">Here\u003C/a>\u003C/p>\n\u003Cp>\u003Cem>Processing time-frequency representations of EEG data using neural networks\u003C/em>\u003C/p>\n\u003Cp>\u003Ca href=\"https://scholar.google.com/citations?view_op=view_citation&#x26;hl=en&#x26;user=WPewiKcAAAAJ&#x26;sortby=pubdate&#x26;citation_for_view=WPewiKcAAAAJ:uWQEDVKXjbEC\">Here\u003C/a>\u003C/p>\n\u003Cp>\u003Cem>Processing EEG data with twin neural networks\u003C/em>\u003C/p>\n\u003Cp>\u003Ca href=\"https://scholar.google.com/citations?view_op=view_citation&#x26;hl=en&#x26;user=WPewiKcAAAAJ&#x26;sortby=pubdate&#x26;citation_for_view=WPewiKcAAAAJ:OU6Ihb5iCvQC\">Here\u003C/a>\u003C/p>\n\u003Cp>\u003Cem>Attention encoding stack in EEG trial aggregation\u003C/em>\u003C/p>\n\u003Cp>\u003Ca href=\"https://scholar.google.com/citations?view_op=view_citation&#x26;hl=en&#x26;user=WPewiKcAAAAJ&#x26;sortby=pubdate&#x26;citation_for_view=WPewiKcAAAAJ:p2g8aNsByqUC\">Here\u003C/a>\u003C/p>\n\u003Cp>\u003Cem>Attention encoding stack in aggregation of data\u003C/em>\u003C/p>\n\u003Cp>\u003Ca href=\"https://scholar.google.com/citations?view_op=view_citation&#x26;hl=en&#x26;user=WPewiKcAAAAJ&#x26;sortby=pubdate&#x26;citation_for_view=WPewiKcAAAAJ:XiSMed-E-HIC\">Here\u003C/a>\u003C/p>\n\u003Cp>\u003Cem>EEG signal representations using auto-encoders\u003C/em>\u003C/p>\n\u003Cp>\u003Ca href=\"https://scholar.google.com/citations?view_op=view_citation&#x26;hl=en&#x26;user=WPewiKcAAAAJ&#x26;sortby=pubdate&#x26;citation_for_view=WPewiKcAAAAJ:u9iWguZQMMsC\">Here\u003C/a>\u003C/p>\n\u003Cp>\u003Cem>Resampling EEG trial data\u003C/em>\u003C/p>\n\u003Cp>\u003Ca href=\"https://scholar.google.com/citations?view_op=view_citation&#x26;hl=en&#x26;user=WPewiKcAAAAJ&#x26;sortby=pubdate&#x26;citation_for_view=WPewiKcAAAAJ:Tiz5es2fbqcC\">Here\u003C/a>\u003C/p>\n\u003Cp>\u003Cem>Processing time-domain and frequency-domain representations of EEG data\u003C/em>\u003C/p>\n\u003Cp>\u003Ca href=\"https://scholar.google.com/citations?view_op=view_citation&#x26;hl=en&#x26;user=WPewiKcAAAAJ&#x26;sortby=pubdate&#x26;citation_for_view=WPewiKcAAAAJ:WbkHhVStYXYCC\">Here\u003C/a>\u003C/p>",{"headings":908,"localImagePaths":909,"remoteImagePaths":910,"frontmatter":911,"imagePaths":912},[],[],[],{"title":899,"excerpt":900,"season":901,"assets":18,"type":9},[],"2022-03-01-amber-patents.md","2022-06-01-proteome-project-patents",{"id":914,"data":916,"body":919,"filePath":920,"digest":921,"rendered":922,"legacyId":930},{"title":917,"excerpt":918,"season":883,"assets":18,"type":9},"Exploratory Interstitial Fluid Project Patents","We shutdown our investigation of a wearable lab and micro-needle array for online proteome analysis. Some patents from the effort apear here.","This project investigated how we could use online extraction and analysis of human interstitial fluid as a technique for monitoring health outcomes like acute kidney injury in humans.  \n\n*Reconstruction of sparse biomedical data* \n\n[Here](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=WPewiKcAAAAJ&sortby=pubdate&citation_for_view=WPewiKcAAAAJ:tOudhMTPpwUC) \n\n*Disease representation and classification with machine learning* \n\n[Here](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=WPewiKcAAAAJ&sortby=pubdate&citation_for_view=WPewiKcAAAAJ:08ZZubdj9fEC)","src/content/blog/2022-06-01-proteome-project-patents.md","c9f6db95577d44c0",{"html":923,"metadata":924},"\u003Cp>This project investigated how we could use online extraction and analysis of human interstitial fluid as a technique for monitoring health outcomes like acute kidney injury in humans.\u003C/p>\n\u003Cp>\u003Cem>Reconstruction of sparse biomedical data\u003C/em>\u003C/p>\n\u003Cp>\u003Ca href=\"https://scholar.google.com/citations?view_op=view_citation&#x26;hl=en&#x26;user=WPewiKcAAAAJ&#x26;sortby=pubdate&#x26;citation_for_view=WPewiKcAAAAJ:tOudhMTPpwUC\">Here\u003C/a>\u003C/p>\n\u003Cp>\u003Cem>Disease representation and classification with machine learning\u003C/em>\u003C/p>\n\u003Cp>\u003Ca href=\"https://scholar.google.com/citations?view_op=view_citation&#x26;hl=en&#x26;user=WPewiKcAAAAJ&#x26;sortby=pubdate&#x26;citation_for_view=WPewiKcAAAAJ:08ZZubdj9fEC\">Here\u003C/a>\u003C/p>",{"headings":925,"localImagePaths":926,"remoteImagePaths":927,"frontmatter":928,"imagePaths":929},[],[],[],{"title":917,"excerpt":918,"season":883,"assets":18,"type":9},[],"2022-06-01-proteome-project-patents.md","2022-12-1-molt5-emnlp2022",{"id":931,"data":933,"filePath":939,"digest":940,"rendered":941,"legacyId":948},{"title":934,"excerpt":935,"season":936,"assets":18,"tags":937,"type":19,"external_url":938},"Translation between Molecules and Natural Language","MolT5 - our work evaluating transformers for text-to-text transfer between natural language and molecular information accepted for oral presentation at EMNLP2022.","winter 2022","machine learning, self-supervised learning, chemistry","https://aclanthology.org/2022.emnlp-main.26/","src/content/blog/2022-12-1-molt5-emnlp2022.md","28c01b20fe1d0813",{"html":24,"metadata":942},{"headings":943,"localImagePaths":944,"remoteImagePaths":945,"frontmatter":946,"imagePaths":947},[],[],[],{"title":934,"excerpt":935,"season":936,"assets":18,"tags":937,"type":19,"external_url":938},[],"2022-12-1-molt5-emnlp2022.md","2024-05-10-lmd3",{"id":949,"data":951,"filePath":956,"digest":957,"rendered":958,"legacyId":965},{"title":952,"excerpt":953,"season":954,"type":19,"external_url":955},"LMD3: Language Model Data Density Dependence","Analyzing language model task performance at the individual example level based on training data density estimation. Presented at CoLM 2024.","spring 2024","https://arxiv.org/abs/2405.06331","src/content/blog/2024-05-10-lmd3.md","fcec9fd190fa6650",{"html":24,"metadata":959},{"headings":960,"localImagePaths":961,"remoteImagePaths":962,"frontmatter":963,"imagePaths":964},[],[],[],{"title":952,"excerpt":953,"season":954,"type":19,"external_url":955},[],"2024-05-10-lmd3.md","2024-04-02-lawinstruct",{"id":966,"data":968,"filePath":973,"digest":974,"rendered":975,"legacyId":982},{"title":969,"excerpt":970,"season":971,"type":19,"external_url":972},"LawInstruct: A Resource for Studying Language Model Adaptation to the Legal Domain","Exploring the effectiveness of instruction-tuning data mixtures for legal reasoning and introducing the LawInstruct dataset. Accepted at NAACL 2025.","spring 2025","https://arxiv.org/abs/2404.02127","src/content/blog/2024-04-02-lawinstruct.md","f1069f231067930f",{"html":24,"metadata":976},{"headings":977,"localImagePaths":978,"remoteImagePaths":979,"frontmatter":980,"imagePaths":981},[],[],[],{"title":969,"excerpt":970,"season":971,"type":19,"external_url":972},[],"2024-04-02-lawinstruct.md","2024-08-01-concept-learning",{"id":983,"data":985,"filePath":990,"digest":991,"rendered":992,"legacyId":999},{"title":986,"excerpt":987,"season":988,"type":19,"external_url":989},"Improving Concept Learning in Education via Category Construction","Investigating the effects of category construction on concept learning in educational settings. Published in the Journal of Educational Psychology.","summer 2024","https://psycnet.apa.org/record/2025-20587-001","src/content/blog/2024-08-01-concept-learning.md","69ef4527adcb6c37",{"html":24,"metadata":993},{"headings":994,"localImagePaths":995,"remoteImagePaths":996,"frontmatter":997,"imagePaths":998},[],[],[],{"title":986,"excerpt":987,"season":988,"type":19,"external_url":989},[],"2024-08-01-concept-learning.md","2025-02-01-joined-deepmind",{"id":1000,"data":1002,"body":1007,"filePath":1008,"digest":1009,"rendered":1010,"legacyId":1018},{"title":1003,"excerpt":1004,"season":1005,"type":9,"tags":1006},"Programming Programs to Program at Google DeepMind","I've joined Google DeepMind as a Staff Research Scientist to work on programming programs to program.","winter 2025","career, google deepmind, gemini","I've joined **Google DeepMind** as a Staff Research Scientist working on advancing the coding capabilities of the **Gemini** models.","src/content/blog/2025-02-01-joined-deepmind.md","8ae184fc42188dcf",{"html":1011,"metadata":1012},"\u003Cp>I’ve joined \u003Cstrong>Google DeepMind\u003C/strong> as a Staff Research Scientist working on advancing the coding capabilities of the \u003Cstrong>Gemini\u003C/strong> models.\u003C/p>",{"headings":1013,"localImagePaths":1014,"remoteImagePaths":1015,"frontmatter":1016,"imagePaths":1017},[],[],[],{"title":1003,"excerpt":1004,"season":1005,"type":9,"tags":1006},[],"2025-02-01-joined-deepmind.md","2025-05-14-jules-io-2025",{"id":1019,"data":1021,"body":1025,"filePath":1026,"digest":1027,"rendered":1028,"legacyId":1036},{"title":1022,"excerpt":1023,"season":971,"type":9,"tags":1024},"Jules: Google's AI Coding Agent Launches at I/O 2025","Launched Jules, an asynchronous coding agent at Google I/O","jules, google io, ai coding","I'm excited to share that we launched **Jules**, Google's AI-powered coding agent at Google I/O 2025! Jules takes you from Github issue to PR with a single prompt.","src/content/blog/2025-05-14-jules-io-2025.md","ec7cba8810b79121",{"html":1029,"metadata":1030},"\u003Cp>I’m excited to share that we launched \u003Cstrong>Jules\u003C/strong>, Google’s AI-powered coding agent at Google I/O 2025! Jules takes you from Github issue to PR with a single prompt.\u003C/p>",{"headings":1031,"localImagePaths":1032,"remoteImagePaths":1033,"frontmatter":1034,"imagePaths":1035},[],[],[],{"title":1022,"excerpt":1023,"season":971,"type":9,"tags":1024},[],"2025-05-14-jules-io-2025.md","2025-07-01-gemini-2-5",{"id":1037,"data":1039,"filePath":1044,"digest":1045,"rendered":1046,"legacyId":1053},{"title":1040,"excerpt":1041,"season":1042,"type":19,"external_url":1043},"Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities","Introducing Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities","summer 2025","https://arxiv.org/abs/2507.06261","src/content/blog/2025-07-01-gemini-2-5.md","b6880b29cb4a5390",{"html":24,"metadata":1047},{"headings":1048,"localImagePaths":1049,"remoteImagePaths":1050,"frontmatter":1051,"imagePaths":1052},[],[],[],{"title":1040,"excerpt":1041,"season":1042,"type":19,"external_url":1043},[],"2025-07-01-gemini-2-5.md"]40,"season":954,"type":9,"tags":1041},[],"2025-05-14-jules-io-2025.md"]