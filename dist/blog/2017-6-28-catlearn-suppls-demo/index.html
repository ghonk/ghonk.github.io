<!DOCTYPE html><html lang="en-us"> <head><meta charset="UTF-8"><meta name="author" content="Garrett Honke"><meta name="viewport" content="width=device-width, initial-scale=1"><link href="http://gmpg.org/xfn/11" rel="profile"><meta http-equiv="X-UA-Compatible" content="IE=edge"><link rel="icon" type="image/x-icon" href="/favicon.ico"><meta name="generator" content="Astro v5.15.9"><title>Getting Started with Catlearn and Catlearn Supplementals &middot; garrett honke</title><meta name="description" content="computational neuroscientist"><!-- Scripts --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"><!-- CSS --><link rel="stylesheet" href="/css/poole.css"><link rel="stylesheet" href="/css/syntax.css"><link rel="stylesheet" href="/css/hyde.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"></head> <body> <div class="sidebar"> <div class="container sidebar-sticky"> <div class="sidebar-about"> <h1> <a href="/">
garrett honke
</a> </h1> <p class="lead">computational neuroscientist</p> </div> <nav class="sidebar-nav"> <a class="sidebar-nav-item" href="/">about</a> <a class="sidebar-nav-item" href="/blog">blog</a> <a class="sidebar-nav-item" href="/cv">cv</a> <a href="http://www.twitter.com/garretthonke" class="fa fa-twitter"></a> <a href="https://github.com/ghonk"><i class="fa fa-github" aria-hidden="true"></i></a> <a href="https://scholar.google.com/citations?user=WPewiKcAAAAJ&hl=en" class="fa fa-graduation-cap"></a> <a href="https://stackexchange.com/users/4275635/ghonke?tab=accounts"><i class="fa fa-stack-overflow" aria-hidden="true"></i></a> <a href="https://www.linkedin.com/in/garretthonke"><i class="fa fa-linkedin" aria-hidden="true"></i></a> </nav> </div> </div> <div class="content container">  <div class="post"> <h1 class="post-title">Getting Started with Catlearn and Catlearn Supplementals</h1> <span class="post-date">summer 2017</span> <h1 id="background">background</h1>
<p>This post is intended to help people get started with their own cognitive modelling or machine learning projects in the <code>catlearn</code> environment with the help of <code>catlearn.suppls</code>, the <a href="http://kurtzlab.psychology.binghamton.edu/">Learning and Representation in Cognition Laboratory</a>’s suite of helper functions for modeling in <code>catlearn</code>.</p>
<p>The motivation behind <code>catlearn.suppls</code> was to create a suite of functions that could be used to rapidly prototype new machine learning architectures that correspond to the cognitive modeling efforts of the LaRC Lab. This includes functions to set up the <em>state</em> information of a model and quickly take rectangular data and format it for the <code>catlearn</code> design pattern---the stateful list processor.</p>
<h1 id="getting-started">getting Started</h1>
<p>If you haven’t already done so, download the <code>catlearn</code> and <code>catlearn.suppls</code> packages.</p>
<p>{% highlight r %}
install.packages(c(“devtools”, “catlearn”))
devtools::install_github(“ghonk/catlearn.suppls”)
{% endhighlight %}</p>
<h1 id="choosing-some-data-and-setting-up-the-model">choosing some data and setting up the model</h1>
<p>We’re going to use the classic <code>iris</code> dataset and DIVA (the DIVergent Autoencoder) for this demonstration. We need two objects to run a model in <code>catlearn</code>, the model’s <code>state</code> and the training matrix, which we will call <code>tr</code>.</p>
<p>{% highlight r %}</p>
<h1 id="--load-the-libraries"># # load the libraries</h1>
<p>library(catlearn)
library(catlearn.suppls)
{% endhighlight %}</p>
<p><em>First, we will construct the model’s state.</em> For this demonstration we’ll set the hyper-parameters to values that we know---a priori---will play nice with our dataset. For real-world problems, you will likely want optimize these values (see future post on grid search and Bayesian optimization options with <code>catlearn.suppls</code>). Detailed description of model hyper-parameters is available in the normal places (e.g., <code>?slpDIVA</code>).</p>
<p>{% highlight r %}</p>
<h1 id="--setup-the-inputs-class-labels-and-model-state"># # setup the inputs, class labels and model state</h1>
<h1 id="check-out-our-data">check out our data</h1>
<p>str(iris)
{% endhighlight %}</p>
<p>{% highlight text %}</p>
<h2 id="dataframe150-obs-of--5-variables">‘data.frame’:	150 obs. of  5 variables:</h2>
<h2 id="-sepallength-num--51-49-47-46-5-54-46-5-44-49">$ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 …</h2>
<h2 id="-sepalwidth--num--35-3-32-31-36-39-34-34-29-31">$ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 …</h2>
<h2 id="-petallength-num--14-14-13-15-14-17-14-15-14-15">$ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 …</h2>
<h2 id="-petalwidth--num--02-02-02-02-02-04-03-02-02-01">$ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 …</h2>
<h2 id="-species------factor-w-3-levels-setosaversicolor-1-1-1-1-1-1-1-1-1-1">$ Species     : Factor w/ 3 levels “setosa”,“versicolor”,..: 1 1 1 1 1 1 1 1 1 1 …</h2>
<p>{% endhighlight %}</p>
<p>{% highlight r %}</p>
<h1 id="find-the-inputs-minus-the-labels">find the inputs minus the labels</h1>
<p>ins &#x3C;- iris[,colnames(iris) != ‘Species’]</p>
<h1 id="create-a-separate-vector-for-the-labels-labels-must-be-numeric">create a separate vector for the labels (labels must be numeric)</h1>
<p>labs &#x3C;- as.numeric(iris[,‘Species’])</p>
<h1 id="get-number-of-categories-and-features">get number of categories and features</h1>
<p>nfeats &#x3C;- dim(ins)[2]
ncats &#x3C;- length(unique(labs))</p>
<h1 id="construct-a-state-list">construct a state list</h1>
<p>st &#x3C;- list(learning_rate = 0.15, num_feats = nfeats, num_hids = 6, num_cats = ncats,
beta_val = 0, phi = 1, continuous = TRUE, in_wts = NULL, out_wts = NULL, wts_range = 1,
wts_center = 0, colskip = 4)
{% endhighlight %}</p>
<p>We can then use <code>catleanr.suppls</code> to create our training matrix</p>
<p>{% highlight r %}</p>
<h1 id="tr_init-initializes-an-empty-training-matrix">tr_init initializes an empty training matrix</h1>
<p>tr &#x3C;- tr_init(nfeats, ncats)</p>
<h1 id="tr_add-fills-in-the-data-and-procedure-ie-training-test-model-reset">tr_add fills in the data and procedure (i.e., training, test, model reset)</h1>
<p>tr &#x3C;- tr_add(inputs = ins, tr = tr, labels = labs, blocks = 12, ctrl = 0,
shuffle = TRUE, reset = TRUE)
{% endhighlight %}</p>
<p>Here’s what our training matrix looks like after the setup procedure:</p>
<p>{% highlight r %}
head(tr)
{% endhighlight %}</p>
<p>{% highlight text %}</p>
<h2 id="ctrl-trial-blk-example--x1--x2--x3--x4-t1-t2-t3">ctrl trial blk example  x1  x2  x3  x4 t1 t2 t3</h2>
<h2 id="1----1-----1---1-----118-77-38-67-22--1--1--1">[1,]    1     1   1     118 7.7 3.8 6.7 2.2 -1 -1  1</h2>
<h2 id="2----0-----2---1------64-61-29-47-14--1--1--1">[2,]    0     2   1      64 6.1 2.9 4.7 1.4 -1  1 -1</h2>
<h2 id="3----0-----3---1------23-46-36-10-02--1--1--1">[3,]    0     3   1      23 4.6 3.6 1.0 0.2  1 -1 -1</h2>
<h2 id="4----0-----4---1------87-67-31-47-15--1--1--1">[4,]    0     4   1      87 6.7 3.1 4.7 1.5 -1  1 -1</h2>
<h2 id="5----0-----5---1------99-51-25-30-11--1--1--1">[5,]    0     5   1      99 5.1 2.5 3.0 1.1 -1  1 -1</h2>
<h2 id="6----0-----6---1------92-61-30-46-14--1--1--1">[6,]    0     6   1      92 6.1 3.0 4.6 1.4 -1  1 -1</h2>
<p>{% endhighlight %}</p>
<p>Finally, we run the model with our state list <code>st</code> and training matrix <code>tr</code></p>
<p>{% highlight r %}
diva_model &#x3C;- slpDIVA(st, tr)
{% endhighlight %}</p>
<p>We can examine performance of the model easily.</p>
<p>{% highlight r %}</p>
<h1 id="--use-response_probs-to-extract-the-response-probabilities"># # use response_probs to extract the response probabilities</h1>
<h1 id="--for-the-target-categories-for-every-training-step-trial"># # for the target categories (for every training step (trial)</h1>
<h1 id="--or-averaged-across-blocks"># # or averaged across blocks)</h1>
<p>response_probs(tr, diva_model$out, blocks = TRUE)
{% endhighlight %}</p>
<p>{% highlight text %}</p>
<h2 id="1-07852448-08431547-08103888-08067568-08087171-08102305-08135465">[1] 0.7852448 0.8431547 0.8103888 0.8067568 0.8087171 0.8102305 0.8135465</h2>
<h2 id="8-08028162-08466575-08375568-08025279-08350697">[8] 0.8028162 0.8466575 0.8375568 0.8025279 0.8350697</h2>
<p>{% endhighlight %}</p>
<p><code>plot_training</code> is a simple function used to plot the learning of one or more models.</p>
<p>{% highlight r %}
plot_training(list(response_probs(tr, diva_model$out, blocks = TRUE)))
{% endhighlight %}</p>
<p><img src="/assets/rfigs/unnamed-chunk-9-1.svg" alt="plot of chunk unnamed-chunk-9"></p>
<p>So with no optimization, we can see that DIVA learns about as much as it is going to learn after one pass through our 150 item training set (obviously absent any cross-validation). Where does it go wrong? You might like to examine which items are not being correctly classified---you can do so by combining the classification probabilities with the original training matrix.</p>
<p>{% highlight r %}</p>
<h1 id="--if-we-want-to-look-at-individual-classication-decisions"># # if we want to look at individual classication decisions,</h1>
<h1 id="--we-can-do-so-by-combining-the-models-output-with-the"># # we can do so by combining the model’s output with the</h1>
<h1 id="--original-training-matrix"># # original training matrix</h1>
<p>trn_result &#x3C;- cbind(tr, round(diva_model$out, 4))
tail(trn_result)
{% endhighlight %}</p>
<p>{% highlight text %}</p>
<h2 id="ctrl-trial-blk-example--x1--x2--x3--x4-t1-t2-t3-1">ctrl trial blk example  x1  x2  x3  x4 t1 t2 t3</h2>
<h2 id="1795----0--1795--12-----123-77-28-67-20--1--1--1-00005-00006-09989">[1795,]    0  1795  12     123 7.7 2.8 6.7 2.0 -1 -1  1 0.0005 0.0006 0.9989</h2>
<h2 id="1796----0--1796--12------82-55-24-37-10--1--1--1-00000-09999-00000">[1796,]    0  1796  12      82 5.5 2.4 3.7 1.0 -1  1 -1 0.0000 0.9999 0.0000</h2>
<h2 id="1797----0--1797--12-----125-67-33-57-21--1--1--1-01204-01765-07030">[1797,]    0  1797  12     125 6.7 3.3 5.7 2.1 -1 -1  1 0.1204 0.1765 0.7030</h2>
<h2 id="1798----0--1798--12-------1-51-35-14-02--1--1--1-10000-00000-00000">[1798,]    0  1798  12       1 5.1 3.5 1.4 0.2  1 -1 -1 1.0000 0.0000 0.0000</h2>
<h2 id="1799----0--1799--12-----144-68-32-59-23--1--1--1-00000-00000-10000">[1799,]    0  1799  12     144 6.8 3.2 5.9 2.3 -1 -1  1 0.0000 0.0000 1.0000</h2>
<h2 id="1800----0--1800--12-----129-64-28-56-21--1--1--1-00001-00002-09996">[1800,]    0  1800  12     129 6.4 2.8 5.6 2.1 -1 -1  1 0.0001 0.0002 0.9996</h2>
<p>{% endhighlight %}</p>
<p>You might also like to see how a number of initializations do on the problem. It’s good practice to average over a series of initializations---something we did not do in this demonstration.</p>
<p>{% highlight r %}</p>
<h1 id="--run-5-models-with-the-same-params-on-the-same-training-set"># # Run 5 models with the same params on the same training set</h1>
<p>model_inits &#x3C;- lapply(1:5, function(x) slpDIVA(st, tr))</p>
<h1 id="--determine-the-response-probability-for-the-correct-class"># # determine the response probability for the correct class</h1>
<p>model_resp_probs &#x3C;-
lapply(model_inits, function(x) {
response_probs(tr, x$out, blocks = TRUE)})</p>
<h1 id="--plot-the-leanrning-curves"># # plot the leanrning curves</h1>
<p>plot_training(model_resp_probs)
{% endhighlight %}</p>
<p><img src="/assets/rfigs/unnamed-chunk-11-1.svg" alt="plot of chunk unnamed-chunk-11"></p>
<p>Here we see that there is a fair amount of variation across initializations. This suggests it would be smart to follow the typical procedure of averaging across a series of models to accurately represent the response probabilities. It also suggests that our approach would likely benefit from some optimization and validation.</p>
<p>Future demos will explore the tools within <code>catlearn.suppls</code> used to optimize hyper-parameters and examine the hidden unit representation space toward the goal of uncovering new insight about the problem.</p> </div>  </div> </body></html>